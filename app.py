from flask import Flask, render_template, jsonify, request
from flask_cors import CORS
import pandas as pd
import numpy as np
import json
import os
from datetime import datetime, timedelta
import requests
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Initialize Flask app
app = Flask(__name__)
CORS(app)

# Initialize models (will be trained on first run)
rf_model = None
scaler = None
feature_cols = None

def create_and_train_models():
    """Create and train advanced flood prediction models with geographic awareness"""
    global rf_model, scaler, feature_cols
    
    print("Creating advanced synthetic training data with geographic factors...")
    
    # Generate comprehensive synthetic flood data
    np.random.seed(42)
    n_samples = 2500  # Increased sample size for better accuracy
    
    # Generate diverse rainfall patterns
    rainfall_1day = np.random.gamma(1.8, 2.5, n_samples)
    rainfall_3day = rainfall_1day + np.random.gamma(1.5, 3.2, n_samples) 
    rainfall_7day = rainfall_3day + np.random.gamma(1.2, 4.5, n_samples)
    
    # Generate elevation and geographic factors
    elevation = np.random.uniform(2, 45, n_samples)
    river_distance = np.random.exponential(15, n_samples)  # Most locations near rivers
    drainage_quality = np.random.choice([0.2, 0.5, 0.8], n_samples, p=[0.3, 0.5, 0.2])  # Good/Moderate/Poor
    
    # Water level calculation with geographic dependencies
    base_water_level = 3.0 + (50 - elevation) / 20  # Lower elevation = higher base level
    rainfall_impact = (rainfall_1day * 0.15 + rainfall_3day * 0.08) * (drainage_quality + 0.5)
    river_impact = np.maximum(0, (10 - river_distance) / 10) * 0.8  # Closer to river = higher levels
    
    water_level_lag1 = base_water_level + rainfall_impact + river_impact + np.random.normal(0, 0.4, n_samples)
    water_level_lag1 = np.maximum(water_level_lag1, 1.5)  # Minimum water level
    
    # Water level trend (momentum)
    water_level_trend = np.random.normal(0, 0.3, n_samples)
    
    # Seasonal effects
    is_monsoon = np.random.choice([0, 1], n_samples, p=[0.65, 0.35])
    monsoon_multiplier = 1 + (is_monsoon * 0.4)  # 40% increase during monsoon
    
    # Apply monsoon effects
    rainfall_1day *= monsoon_multiplier
    rainfall_3day *= monsoon_multiplier
    rainfall_7day *= monsoon_multiplier
    water_level_lag1 *= (1 + is_monsoon * 0.2)
    
    # Geographic risk factor
    geographic_risk = (
        (50 - elevation) / 50 * 0.35 +  # Elevation factor
        np.maximum(0, (20 - river_distance) / 20) * 0.4 +  # River proximity
        drainage_quality * 0.25  # Drainage quality
    )
    geographic_risk = np.clip(geographic_risk, 0.1, 0.9)
    
    # Create comprehensive feature matrix (9 features)
    X = np.column_stack([
        rainfall_1day, rainfall_3day, rainfall_7day,
        water_level_lag1, water_level_trend, is_monsoon,
        elevation, river_distance, geographic_risk
    ])
    
    feature_cols = ['rainfall_1day', 'rainfall_3day', 'rainfall_7day', 
                   'water_level_lag1', 'water_level_trend', 'is_monsoon',
                   'elevation', 'river_distance', 'geographic_risk']
    
    # Create realistic flood labels with multiple conditions
    high_rainfall_flood = (rainfall_3day > 12) & (drainage_quality > 0.4)  # Heavy rain + poor drainage
    water_level_flood = water_level_lag1 > (5.0 + elevation * 0.05)  # Dynamic threshold by elevation
    geographic_flood = (geographic_risk > 0.7) & (rainfall_1day > 6)  # High risk areas with moderate rain
    monsoon_flood = (is_monsoon == 1) & (rainfall_7day > 25) & (elevation < 15)  # Monsoon flooding
    
    y = (high_rainfall_flood | water_level_flood | geographic_flood | monsoon_flood).astype(int)
    
    print(f"Advanced training data: {X.shape[0]} samples, {y.sum()} flood events ({y.mean()*100:.1f}%)")
    print(f"Feature distribution: {X.shape[1]} features including geographic factors")
    
    # Split with stratification to ensure balanced test set
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Advanced scaling
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Train enhanced Random Forest with better parameters
    rf_model = RandomForestClassifier(
        n_estimators=200,  # More trees for better accuracy
        max_depth=12,      # Prevent overfitting
        min_samples_split=5,
        min_samples_leaf=3,
        max_features='sqrt',
        class_weight='balanced',  # Handle class imbalance
        random_state=42
    )
    rf_model.fit(X_train_scaled, y_train)
    
    # Comprehensive evaluation
    from sklearn.metrics import classification_report, confusion_matrix
    
    train_accuracy = rf_model.score(X_train_scaled, y_train)
    test_accuracy = rf_model.score(X_test_scaled, y_test)
    y_pred = rf_model.predict(X_test_scaled)
    
    print(f"Advanced model trained!")
    print(f"Training accuracy: {train_accuracy:.3f}")
    print(f"Test accuracy: {test_accuracy:.3f}")
    print(f"Overfitting check: {abs(train_accuracy - test_accuracy):.3f} (should be < 0.1)")
    
    # Feature importance analysis
    feature_importance = rf_model.feature_importances_
    importance_dict = dict(zip(feature_cols, feature_importance))
    print("Feature importance:")
    for feature, importance in sorted(importance_dict.items(), key=lambda x: x[1], reverse=True):
        print(f"  {feature}: {importance:.3f}")
    
    # Save models with version info
    os.makedirs('models', exist_ok=True)
    model_metadata = {
        'version': '2.0.0',
        'features': feature_cols,
        'accuracy': test_accuracy,
        'samples': n_samples,
        'created': datetime.now().isoformat()
    }
    
    joblib.dump(rf_model, 'models/rf_flood_model.pkl')
    joblib.dump(scaler, 'models/feature_scaler.pkl')
    joblib.dump(feature_cols, 'models/feature_columns.pkl')
    joblib.dump(model_metadata, 'models/model_metadata.pkl')
    
    return rf_model, scaler, feature_cols

# Load or create models
try:
    # Try to load existing models (will fail on first deployment)
    rf_model = joblib.load('models/rf_flood_model.pkl')
    scaler = joblib.load('models/feature_scaler.pkl')
    feature_cols = joblib.load('models/feature_columns.pkl')
    print("✅ Models loaded successfully from files!")
except Exception as e:
    print(f"⚠️ Model files not found, training new models: {str(e)}")
    try:
        rf_model, scaler, feature_cols = create_and_train_models()
        print("✅ New models trained successfully!")
    except Exception as training_error:
        print(f"❌ Error training models: {str(training_error)}")
        # Set default models to prevent app crash
        rf_model = None
        scaler = None
        feature_cols = None

# Configuration
OPENWEATHER_API_KEY = os.environ.get('OPENWEATHER_API_KEY', '4d6eb4cfda31ca9dd9e06e83566e0e7a')
OPENWEATHER_BASE_URL = "https://api.openweathermap.org/data/2.5"

LOCATIONS = {
    'Dhaka': (23.8103, 90.4125),
    'Sylhet': (24.8949, 91.8687),
    'Rangpur': (25.7439, 89.2752),
    'Bahadurabad': (25.1906, 89.7006),
    'Chittagong': (22.3569, 91.7832)
}

FLOOD_THRESHOLDS = {
    'Dhaka': 5.5,
    'Sylhet': 6.0,
    'Rangpur': 4.8,
    'Bahadurabad': 7.2,
    'Chittagong': 3.5
}

# Enhanced geographic data with more accurate real-world information
GEOGRAPHIC_DATA = {
    'Dhaka': {
        'elevation': 8.2,  # meters above sea level (accurate)
        'distance_to_major_river': 1.8,  # km to Buriganga River
        'drainage_quality': 'Poor',  # Known urban drainage issues
        'river_confluence_distance': 12.5,  # km to Padma-Meghna confluence
        'topography': 'low_lying_urban',
        'base_risk_factor': 0.02,  # Ultra-ultra-low baseline (2-8% max on sunny days)
        'annual_rainfall_mm': 2025,  # Historical average
        'flood_history_frequency': 8,  # floods per decade
        'population_density': 23234,  # people per km²
        'urbanization_factor': 0.95,  # Highly urbanized
        'soil_type': 'clay_alluvial',  # Poor drainage soil
        'river_systems': ['Buriganga', 'Turag', 'Balu', 'Shitalakhya']
    },
    'Sylhet': {
        'elevation': 11.8,
        'distance_to_major_river': 0.2,  # Directly on Surma river
        'drainage_quality': 'Moderate',
        'river_confluence_distance': 25.3,  # Distance to major confluence
        'topography': 'river_valley',
        'base_risk_factor': 0.025,  # Ultra-ultra-low baseline for flood-prone area
        'annual_rainfall_mm': 3334,  # One of highest in Bangladesh
        'flood_history_frequency': 12,  # Very frequent flooding
        'population_density': 1020,
        'urbanization_factor': 0.65,
        'soil_type': 'sandy_alluvial',  # Better drainage than clay
        'river_systems': ['Surma', 'Kushiyara', 'Manu']
    },
    'Rangpur': {
        'elevation': 32.5,  # Higher elevation, northern region
        'distance_to_major_river': 58.7,  # Far from major rivers
        'drainage_quality': 'Good',
        'river_confluence_distance': 89.4,
        'topography': 'elevated_plain',
        'base_risk_factor': 0.015,  # Ultra-ultra-minimal baseline for safe area
        'annual_rainfall_mm': 1448,  # Lower rainfall region
        'flood_history_frequency': 3,  # Less frequent flooding
        'population_density': 1265,
        'urbanization_factor': 0.45,
        'soil_type': 'loamy',  # Good drainage
        'river_systems': ['Teesta', 'Karatoya']
    },
    'Bahadurabad': {
        'elevation': 16.3,
        'distance_to_major_river': 0.8,  # Very close to Jamuna/Brahmaputra
        'drainage_quality': 'Moderate',
        'river_confluence_distance': 8.2,  # Near major confluence point
        'topography': 'river_plain',
        'base_risk_factor': 0.025,  # Ultra-ultra-low baseline for river proximity
        'annual_rainfall_mm': 1832,
        'flood_history_frequency': 9,  # Frequent due to Jamuna flooding
        'population_density': 890,
        'urbanization_factor': 0.35,
        'soil_type': 'silty_alluvial',
        'river_systems': ['Jamuna', 'Brahmaputra', 'Old Brahmaputra']
    },
    'Chittagong': {
        'elevation': 5.8,  # Coastal, very low elevation
        'distance_to_major_river': 2.1,  # Near Karnaphuli River
        'drainage_quality': 'Poor',  # Coastal drainage challenges
        'river_confluence_distance': 42.8,
        'topography': 'coastal_low',
        'base_risk_factor': 0.02,  # Ultra-ultra-low baseline for coastal area
        'annual_rainfall_mm': 2666,  # High coastal rainfall
        'flood_history_frequency': 7,  # Regular coastal and river flooding
        'population_density': 2800,
        'urbanization_factor': 0.78,
        'soil_type': 'sandy_clay',  # Mixed coastal soil
        'river_systems': ['Karnaphuli', 'Halda', 'Sangu']
    }
}

@app.route('/')
def dashboard():
    """Main dashboard page - API status"""
    return jsonify({
        "message": "AI Flood Prediction System API",
        "status": "live",
        "version": "1.0.0",
        "endpoints": [
            "/api/locations",
            "/api/predict/<location>",
            "/api/predict/coordinates/<lat>/<lon>",
            "/api/history/<location>",
            "/api/alerts",
            "/api/status"
        ]
    })

@app.route('/api/locations')
def get_locations():
    """Get all monitored locations"""
    locations_data = []
    for name, (lat, lon) in LOCATIONS.items():
        locations_data.append({
            'name': name,
            'lat': lat,
            'lon': lon,
            'threshold': FLOOD_THRESHOLDS.get(name, 5.5)
        })
    return jsonify(locations_data)

@app.route('/api/predict/<location>')
def predict_location(location):
    """Get highly accurate flood prediction for a specific location"""
    if location not in LOCATIONS:
        return jsonify({'error': 'Location not found'}), 404
    
    try:
        # Fetch comprehensive weather data
        weather_data = fetch_real_weather_data(location, days=7)
        
        # Get enhanced geographic risk factors
        geographic_risk = calculate_enhanced_geographic_risk(location)
        flood_risk_profile = calculate_flood_risk_profile(location)
        
        # Calculate overall risk as weighted average of individual flood types
        weighted_overall_risk = calculate_weighted_overall_risk(flood_risk_profile)
        
        geo_data = GEOGRAPHIC_DATA.get(location, {})
        base_risk = geo_data.get('base_risk_factor', 0.5)
        
        # Create enhanced DataFrame with all factors
        live_data = weather_data.copy()
        
        # Calculate realistic water levels using multiple factors
        elevation = geo_data.get('elevation', 10)
        drainage_quality = geo_data.get('drainage_quality', 'Moderate')
        distance_to_river = geo_data.get('distance_to_major_river', 5)
        urbanization = geo_data.get('urbanization_factor', 0.5)
        
        # Enhanced water level calculation
        elevation_factor = max(0.3, 1.0 - (elevation / 50.0))  # Lower elevation = higher base level
        drainage_multiplier = {'Poor': 1.4, 'Moderate': 1.1, 'Good': 0.8}.get(drainage_quality, 1.1)
        river_proximity_factor = max(0.5, 1.0 - (distance_to_river / 20.0))
        urban_runoff_factor = 1.0 + (urbanization * 0.3)  # Urban areas have more runoff
        
        # Base water level adjusted for all factors
        base_water_level = (2.8 + elevation_factor * 2.2 + 
                           river_proximity_factor * 0.8)
        
        # Calculate daily water levels with realistic modeling
        water_levels = []
        for i, rainfall in enumerate(live_data['rainfall']):
            # Cumulative effect of recent rainfall
            recent_rain_effect = 0
            for j in range(max(0, i-2), i+1):  # 3-day influence
                days_ago = i - j
                decay_factor = 0.7 ** days_ago  # Exponential decay
                if j < len(live_data):
                    recent_rain_effect += live_data['rainfall'].iloc[j] * decay_factor
            
            # Daily water level calculation
            daily_level = (base_water_level + 
                          (rainfall * 0.08 * drainage_multiplier * urban_runoff_factor) +
                          (recent_rain_effect * 0.04 * drainage_multiplier) +
                          np.random.normal(0, 0.12))  # Natural variation
            
            water_levels.append(max(daily_level, 1.8))  # Minimum realistic level
        
        live_data['estimated_water_level'] = water_levels
        
        # Current conditions
        latest_rainfall = live_data['rainfall'].iloc[-1]
        latest_water_level = live_data['estimated_water_level'].iloc[-1]
        threshold = FLOOD_THRESHOLDS.get(location, 5.5)
        
        # Calculate basic aggregates needed for both ML and fallback
        rainfall_3day = live_data['rainfall'].tail(3).sum()
        rainfall_7day = live_data['rainfall'].sum()
        
        # Enhanced ML prediction with all 9 features
        if rf_model is not None and scaler is not None and len(feature_cols) == 9:
            try:
                # Calculate comprehensive features
                rainfall_1day = latest_rainfall
                # rainfall_3day and rainfall_7day already calculated above
                water_level_lag1 = latest_water_level
                
                # Water level trend (slope over last 3 days)
                if len(live_data) >= 3:
                    recent_levels = live_data['estimated_water_level'].tail(3).values
                    water_level_trend = (recent_levels[-1] - recent_levels[0]) / 2
                else:
                    water_level_trend = 0
                
                # Seasonal factor
                current_month = datetime.now().month
                is_monsoon = 1 if 6 <= current_month <= 9 else 0
                
                # Geographic features
                river_distance = distance_to_river
                
                # Create 9-feature array to match training
                features = np.array([[
                    rainfall_1day, rainfall_3day, rainfall_7day,
                    water_level_lag1, water_level_trend, is_monsoon,
                    elevation, river_distance, geographic_risk
                ]])
                
                # Get ML prediction
                features_scaled = scaler.transform(features)
                ml_risk_probability = rf_model.predict_proba(features_scaled)[0, 1]
                
                # Confidence based on feature consistency
                feature_ranges = {
                    'rainfall_1day': [0, 30],
                    'rainfall_3day': [0, 80],
                    'elevation': [0, 50],
                    'river_distance': [0, 100]
                }
                
                confidence_factors = []
                for i, feature_name in enumerate(['rainfall_1day', 'rainfall_3day', 'elevation', 'river_distance']):
                    if i < len(features[0]) and feature_name in feature_ranges:
                        feature_val = features[0][i]
                        min_val, max_val = feature_ranges[feature_name]
                        # Higher confidence when features are in expected ranges
                        if min_val <= feature_val <= max_val:
                            confidence_factors.append(0.9)
                        else:
                            confidence_factors.append(0.6)
                
                model_confidence = np.mean(confidence_factors) if confidence_factors else 0.8
                
                # Apply temporal consistency (smooth transitions)
                temporal_smoothing = 0.82
                historical_risk_estimate = geographic_risk  # Use as baseline
                
                # Extremely conservative risk calculation to prevent inflated values
                # Start with very low base component weights
                base_ml_risk = ml_risk_probability * model_confidence
                
                # Apply ultra-conservative risk scaling - ensure sunny days stay at 5-10%
                # Only extreme conditions should show moderate to high risk
                if base_ml_risk < 0.15:
                    # Very low risk scenarios (most common) - keep them ultra-low
                    final_risk_score = base_ml_risk * 0.25 + geographic_risk * 0.06
                elif base_ml_risk < 0.35:
                    # Low risk scenarios - minimal increase
                    final_risk_score = base_ml_risk * 0.35 + geographic_risk * 0.08
                else:
                    # Moderate risk scenarios - gentle increase
                    final_risk_score = base_ml_risk * 0.45 + geographic_risk * 0.10
                
                # Ultra-conservative extreme conditions check
                extreme_rain = rainfall_3day > (geo_data.get('annual_rainfall_mm', 2000) / 20)  # More than 5% of annual rain in 3 days
                extreme_water = latest_water_level > (threshold * 0.90)
                
                # Tiny boosts for extreme conditions
                if extreme_rain and extreme_water:
                    final_risk_score += 0.03  # Very small boost only when both conditions are extreme
                elif extreme_rain or extreme_water:
                    final_risk_score += 0.01  # Minimal boost for single extreme condition
                
                # Apply ultra-conservative temporal smoothing
                temporal_smoothing = 0.70
                ultra_conservative_base = min(base_risk, 0.08)  # Even lower cap on base risk influence
                final_risk_score = (final_risk_score * temporal_smoothing + 
                                  ultra_conservative_base * (1 - temporal_smoothing))
                
                # Ultra-conservative bounds - ensure sunny days are 5-10% max
                min_risk = 0.02  # Lower minimum
                max_risk = 0.45   # Lower maximum
                final_risk_score = np.clip(final_risk_score, min_risk, max_risk)
                
                # Override ML risk with weighted overall risk from flood types
                final_risk_score = weighted_overall_risk
                
                flood_prediction = int(final_risk_score > 0.6)
                
                # Enhanced debugging info
                debug_info = {
                    'ml_risk_probability': float(ml_risk_probability),
                    'model_confidence': float(model_confidence),
                    'temporal_smoothing_applied': temporal_smoothing,
                    'extreme_conditions': {
                        'extreme_rain': extreme_rain,
                        'extreme_water': extreme_water
                    },
                    'features_used': dict(zip(feature_cols, features[0])),
                    'risk_components': {
                        'ml_component': float(ml_risk_probability * 0.70 * model_confidence),
                        'geographic_component': float(geographic_risk * 0.20),
                        'historical_component': float(historical_risk_estimate * 0.10)
                    }
                }
                
            except Exception as e:
                print(f"Enhanced ML prediction error: {e}")
                # Robust fallback calculation
                final_risk_score = calculate_fallback_risk(location, latest_rainfall, rainfall_3day, 
                                                         latest_water_level, threshold, geographic_risk)
                flood_prediction = int(final_risk_score > 0.6)
                debug_info = {'error': str(e), 'used_fallback': True}
        else:
            # Enhanced fallback calculation
            final_risk_score = calculate_fallback_risk(location, latest_rainfall, rainfall_3day, 
                                                     latest_water_level, threshold, geographic_risk)
            flood_prediction = int(final_risk_score > 0.6)
            debug_info = {'used_fallback': True, 'reason': 'Model not available or incomplete features'}
        
        # Override final_risk_score with weighted overall risk from flood types
        final_risk_score = weighted_overall_risk
        flood_prediction = int(final_risk_score > 0.6)
        
        # Determine risk level with enhanced granularity
        if final_risk_score >= 0.85:
            status = 'EXTREME RISK'
            risk_class = 'risk-extreme'
        elif final_risk_score >= 0.75:
            status = 'CRITICAL RISK'
            risk_class = 'risk-critical'
        elif final_risk_score >= 0.6:
            status = 'HIGH RISK'
            risk_class = 'risk-high'
        elif final_risk_score >= 0.4:
            status = 'MODERATE RISK'
            risk_class = 'risk-medium'
        elif final_risk_score >= 0.2:
            status = 'LOW RISK'
            risk_class = 'risk-low'
        else:
            status = 'MINIMAL RISK'
            risk_class = 'risk-minimal'
        
        # Calculate prediction confidence
        prediction_confidence = max(final_risk_score, 1-final_risk_score)
        
        # Create comprehensive response with enhanced data
        response_data = {
            'location': location,
            'timestamp': datetime.now().isoformat(),
            'current_rainfall': float(latest_rainfall),
            'current_water_level': float(latest_water_level),
            'flood_threshold': float(threshold),
            'flood_risk': int(flood_prediction),
            'risk_probability': float(final_risk_score),
            'confidence': float(prediction_confidence),
            'status': status,
            'risk_class': risk_class,
            'geographic_factors': {
                'elevation_m': geo_data.get('elevation', 0),
                'distance_to_river_km': geo_data.get('distance_to_major_river', 0),
                'drainage_quality': geo_data.get('drainage_quality', 'Unknown'),
                'topography': geo_data.get('topography', 'Unknown'),
                'urbanization_factor': geo_data.get('urbanization_factor', 0.5),
                'annual_rainfall_mm': geo_data.get('annual_rainfall_mm', 2000),
                'flood_history_frequency': geo_data.get('flood_history_frequency', 5),
                'soil_type': geo_data.get('soil_type', 'Unknown'),
                'river_systems': geo_data.get('river_systems', []),
                'base_risk_factor': float(base_risk),
                'geographic_risk_contribution': float(geographic_risk)
            },
            'flood_risk_profile': {
                flood_type: {
                    'risk_percentage': float(profile['risk_percentage']),
                    'severity_level': profile['severity_level'],
                    'flood_degree': profile['flood_degree'],
                    'estimated_depth': profile['estimated_depth'],
                    'description': profile['description'],
                    'typical_damage': profile['typical_damage']
                } for flood_type, profile in flood_risk_profile.items()
            },
            'recent_data': [{
                'date': row['date'],
                'rainfall': float(row['rainfall']),
                'estimated_water_level': float(row['estimated_water_level'])
            } for _, row in live_data.iterrows()],
            'model_info': {
                'version': '2.0.0',
                'features_count': len(feature_cols) if feature_cols else 0,
                'prediction_method': 'enhanced_ml' if rf_model is not None else 'fallback',
                'debug': debug_info
            }
        }
        
        # Log prediction with enhanced details
        log_prediction(location, response_data)
        
        return jsonify(response_data)
        
    except Exception as e:
        print(f"Prediction error for {location}: {str(e)}")
        return jsonify({'error': str(e), 'location': location}), 500

@app.route('/api/history/<location>')
def get_history(location):
    """Get prediction history for a location"""
    log_file = 'logs/flood_predictions.csv'
    
    # Create sample historical data if no log file exists
    if not os.path.exists(log_file):
        create_sample_history()
    
    try:
        df = pd.read_csv(log_file)
        location_history = df[df['location'] == location].tail(30)
        
        history = []
        for _, row in location_history.iterrows():
            history.append({
                'date': row['date'],
                'rainfall': float(row['rainfall']),
                'water_level': float(row['water_level']),
                'prediction': int(row['flood_risk']),
                'probability': float(row['risk_probability'])
            })
        
        return jsonify({'history': history})
        
    except Exception as e:
        return jsonify({'error': str(e), 'history': []}), 500

def create_sample_history():
    """Create sample historical prediction data for demonstration"""
    log_file = 'logs/flood_predictions.csv'
    
    # Generate 30 days of sample data
    sample_data = []
    base_date = datetime.now() - timedelta(days=30)
    
    for i in range(30):
        current_date = base_date + timedelta(days=i)
        
        for location in LOCATIONS.keys():
            # Generate realistic sample data
            np.random.seed(i + hash(location) % 1000)
            
            rainfall = max(0, np.random.gamma(2, 3) + np.random.normal(0, 2))
            water_level = 4.0 + (rainfall * 0.08) + np.random.normal(0, 0.3)
            water_level = max(water_level, 2.0)
            
            threshold = FLOOD_THRESHOLDS.get(location, 5.5)
            
            # Simple risk calculation
            risk_factors = [
                water_level / threshold,
                rainfall / 20,
                min(rainfall / 10, 1.0)
            ]
            risk_prob = np.clip(np.mean(risk_factors), 0, 1)
            flood_risk = 1 if risk_prob > 0.6 else 0
            
            sample_data.append({
                'timestamp': current_date.strftime('%Y-%m-%d %H:%M:%S'),
                'location': location,
                'date': current_date.strftime('%Y-%m-%d'),
                'rainfall': round(rainfall, 1),
                'water_level': round(water_level, 2),
                'flood_threshold': threshold,
                'flood_risk': flood_risk,
                'risk_probability': round(risk_prob, 3),
                'confidence': round(max(risk_prob, 1-risk_prob), 3),
                'status': 'HIGH RISK' if flood_risk == 1 else 'LOW RISK'
            })
    
    # Save to CSV
    os.makedirs('logs', exist_ok=True)
    df = pd.DataFrame(sample_data)
    df.to_csv(log_file, index=False)
    print(f"📊 Created sample historical data: {len(sample_data)} records")

def log_prediction(location, prediction_data):
    """Log a prediction to the history file"""
    log_file = 'logs/flood_predictions.csv'
    
    log_entry = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'location': location,
        'date': datetime.now().strftime('%Y-%m-%d'),
        'rainfall': prediction_data.get('current_rainfall', 0),
        'water_level': prediction_data.get('current_water_level', 0),
        'flood_threshold': prediction_data.get('flood_threshold', 5.5),
        'flood_risk': prediction_data.get('flood_risk', 0),
        'risk_probability': prediction_data.get('risk_probability', 0),
        'confidence': prediction_data.get('confidence', 0),
        'status': prediction_data.get('status', 'LOW RISK')
    }
    
    log_df = pd.DataFrame([log_entry])
    
    if os.path.exists(log_file):
        log_df.to_csv(log_file, mode='a', header=False, index=False)
    else:
        os.makedirs('logs', exist_ok=True)
        log_df.to_csv(log_file, mode='w', header=True, index=False)

@app.route('/api/alerts')
def get_alerts():
    """Get recent alerts"""
    alert_file = 'alerts/alert_history.csv'
    
    if not os.path.exists(alert_file):
        return jsonify({'alerts': []})
    
    try:
        df = pd.read_csv(alert_file)
        recent_alerts = df.tail(10)
        
        alerts = []
        for _, row in recent_alerts.iterrows():
            alerts.append({
                'timestamp': row['timestamp'],
                'location': row['location'],
                'alert_type': row['alert_type'],
                'risk_probability': round(row['risk_probability'], 3)
            })
        
        return jsonify({'alerts': alerts})
        
    except Exception as e:
        return jsonify({'error': str(e), 'alerts': []}), 500

@app.route('/api/status')
def system_status():
    """Get system status"""
    return jsonify({
        'status': 'operational',
        'models_loaded': rf_model is not None and scaler is not None,
        'last_update': datetime.now().isoformat(),
        'monitored_locations': len(LOCATIONS),
        'version': '1.0.0'
    })

def fetch_real_weather_data(location='Dhaka', days=7):
    """Fetch real weather data from OpenWeatherMap API"""
    if not OPENWEATHER_API_KEY or OPENWEATHER_API_KEY == 'your_openweather_api_key':
        print("⚠️ Using simulated data - OpenWeatherMap API key not configured")
        return get_simulated_data(location, days)
    
    lat, lon = LOCATIONS.get(location, LOCATIONS['Dhaka'])
    
    try:
        # Get current weather
        current_url = f"{OPENWEATHER_BASE_URL}/weather?lat={lat}&lon={lon}&appid={OPENWEATHER_API_KEY}&units=metric"
        response = requests.get(current_url, timeout=10)
        
        if response.status_code == 200:
            current_data = response.json()
            
            # Extract current rainfall (if available)
            current_rain = 0
            if 'rain' in current_data:
                current_rain = current_data['rain'].get('1h', 0)  # mm in last hour
            
            # For historical data, we'll simulate based on current conditions
            dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days)]
            dates.reverse()
            
            # Generate realistic variations around current conditions
            base_rainfall = max(current_rain * 24, 1)  # Convert hourly to daily estimate
            rainfall_data = []
            
            for i in range(days):
                # Add some realistic variation
                daily_rain = base_rainfall * (0.5 + np.random.random()) * (1 + 0.3 * np.sin(i * 0.5))
                rainfall_data.append(max(daily_rain, 0))
            
            return pd.DataFrame({
                'date': dates,
                'rainfall': rainfall_data
            })
        else:
            print(f"⚠️ Weather API error: {response.status_code}, using simulated data")
            return get_simulated_data(location, days)
            
    except Exception as e:
        print(f"⚠️ Weather API exception: {str(e)}, using simulated data")
        return get_simulated_data(location, days)

def get_simulated_data(location='Dhaka', days=7):
    """Generate simulated rainfall data as fallback"""
    np.random.seed(int(datetime.now().timestamp()) % 1000)
    
    dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days)]
    dates.reverse()
    
    # Simulate realistic rainfall patterns
    rainfall = np.random.gamma(2, 3, days)
    rainfall = np.maximum(rainfall, 0)
    
    return pd.DataFrame({
        'date': dates,
        'rainfall': rainfall
    })

def calculate_flood_risk_profile(location, lat=None, lon=None):
    """Calculate comprehensive flood risk profile with type-specific assessments and flood degrees"""
    geo_data = GEOGRAPHIC_DATA.get(location, {}) if location else {}
    
    # Base geographic factors
    elevation = geo_data.get('elevation', 10)
    river_distance = geo_data.get('distance_to_major_river', 10)
    drainage_quality = geo_data.get('drainage_quality', 'Moderate')
    urbanization = geo_data.get('urbanization_factor', 0.5)
    flood_frequency = geo_data.get('flood_history_frequency', 5)
    
    # For coordinate-based predictions, use interpolated data if available
    if lat is not None and lon is not None and (location is None or not geo_data):
        # Use basic interpolation for coordinates
        elevation = max(5, 15 - abs(lat - 23.5) * 2)  # Rough elevation estimate
        river_distance = min(50, abs(lat - 23.5) * 10 + abs(lon - 90.0) * 8)
        urbanization = max(0.2, 0.8 - (abs(lat - 23.68) + abs(lon - 90.35)) * 0.3)
        drainage_quality = 'Moderate'  # Default for coordinates
        flood_frequency = 5  # Default for coordinates
    
    # Calculate different flood type risks with ultra-conservative approach
    flood_types = {}
    
    def get_flood_degree(risk_value):
        """Determine flood degree based on risk value"""
        if risk_value >= 0.20:
            return "Severe"
        elif risk_value >= 0.12:
            return "Moderate"
        elif risk_value >= 0.06:
            return "Minor"
        else:
            return "Minimal"
    
    def get_water_depth_estimate(flood_type, risk_value):
        """Estimate potential water depth based on flood type and risk"""
        base_depths = {
            'riverine': {'Severe': '2-4m', 'Moderate': '1-2m', 'Minor': '0.3-1m', 'Minimal': '0-0.3m'},
            'urban_drainage': {'Severe': '1-2m', 'Moderate': '0.5-1m', 'Minor': '0.2-0.5m', 'Minimal': '0-0.2m'},
            'flash': {'Severe': '1.5-3m', 'Moderate': '0.8-1.5m', 'Minor': '0.3-0.8m', 'Minimal': '0-0.3m'},
            'tidal': {'Severe': '2-5m', 'Moderate': '1-2m', 'Minor': '0.5-1m', 'Minimal': '0-0.5m'}
        }
        degree = get_flood_degree(risk_value)
        return base_depths.get(flood_type, {}).get(degree, '0-0.3m')
    
    # 1. RIVERINE FLOODING (major river overflow) - Ultra-conservative
    riverine_risk = max(0.01, 1.0 - (river_distance / 25.0))  # Reduced from 20.0
    riverine_risk *= (1.0 - elevation / 40.0)  # Reduced from 30.0
    if flood_frequency > 10:  # Raised threshold from 8
        riverine_risk *= 1.05  # Reduced from 1.1
    final_riverine_risk = np.clip(riverine_risk * 0.15, 0.01, 0.15)  # Reduced from 0.4 and 0.35
    
    flood_types['riverine'] = {
        'risk_percentage': final_riverine_risk * 100,
        'severity_level': 'High' if final_riverine_risk > 0.12 else 'Moderate' if final_riverine_risk > 0.06 else 'Low',
        'flood_degree': get_flood_degree(final_riverine_risk),
        'estimated_depth': get_water_depth_estimate('riverine', final_riverine_risk),
        'description': 'Major river overflow causing widespread inundation',
        'typical_damage': 'Infrastructure damage, displacement, agricultural losses'
    }
    
    # 2. URBAN DRAINAGE FLOODING (poor drainage, clogged systems) - Ultra-conservative
    urban_drainage_risk = urbanization * 0.5  # Reduced from 0.8
    drainage_multiplier = {'Poor': 1.2, 'Moderate': 0.9, 'Good': 0.5}.get(drainage_quality, 0.9)  # Reduced all values
    urban_drainage_risk *= drainage_multiplier
    if location in ['Dhaka', 'Chittagong']:  # Known urban drainage issues
        urban_drainage_risk *= 1.05  # Reduced from 1.2
    final_urban_risk = np.clip(urban_drainage_risk * 0.12, 0.01, 0.12)  # Reduced from 0.3 and 0.25
    
    flood_types['urban_drainage'] = {
        'risk_percentage': final_urban_risk * 100,
        'severity_level': 'High' if final_urban_risk > 0.09 else 'Moderate' if final_urban_risk > 0.05 else 'Low',
        'flood_degree': get_flood_degree(final_urban_risk),
        'estimated_depth': get_water_depth_estimate('urban_drainage', final_urban_risk),
        'description': 'Waterlogging due to inadequate drainage systems',
        'typical_damage': 'Traffic disruption, property damage, health concerns'
    }
    
    # 3. FLASH FLOODING (sudden, intense rainfall) - Ultra-conservative
    flash_risk = (1.0 - elevation / 35.0) * 0.4  # Reduced from 25.0 and 0.6
    if geo_data.get('topography') in ['low_lying_urban', 'river_valley']:
        flash_risk *= 1.1  # Reduced from 1.3
    final_flash_risk = np.clip(flash_risk * 0.18, 0.01, 0.14)  # Reduced from 0.35 and 0.3
    
    flood_types['flash'] = {
        'risk_percentage': final_flash_risk * 100,
        'severity_level': 'High' if final_flash_risk > 0.10 else 'Moderate' if final_flash_risk > 0.05 else 'Low',
        'flood_degree': get_flood_degree(final_flash_risk),
        'estimated_depth': get_water_depth_estimate('flash', final_flash_risk),
        'description': 'Rapid onset flooding from intense rainfall',
        'typical_damage': 'Immediate safety risks, transportation disruption'
    }
    
    # 4. TIDAL/COASTAL FLOODING (for coastal areas) - Ultra-conservative
    tidal_risk = 0.01  # Reduced from 0.02
    if location in ['Chittagong'] or geo_data.get('topography') == 'coastal_low':
        tidal_risk = max(0.08, 1.0 - elevation / 12.0)  # Reduced from 0.15 and 8.0
    final_tidal_risk = np.clip(tidal_risk * 0.12, 0.01, 0.10)  # Reduced from 0.25 and 0.2
    
    flood_types['tidal'] = {
        'risk_percentage': final_tidal_risk * 100,
        'severity_level': 'High' if final_tidal_risk > 0.08 else 'Moderate' if final_tidal_risk > 0.04 else 'Low',
        'flood_degree': get_flood_degree(final_tidal_risk),
        'estimated_depth': get_water_depth_estimate('tidal', final_tidal_risk),
        'description': 'Sea level rise and storm surge flooding',
        'typical_damage': 'Saltwater contamination, infrastructure corrosion'
    }
    
    return flood_types

def calculate_enhanced_geographic_risk(location):
    """Calculate enhanced geographic risk factor with comprehensive analysis"""
    if location not in GEOGRAPHIC_DATA:
        return 0.5  # Default moderate risk
    
    geo_data = GEOGRAPHIC_DATA[location]
    
    # 1. Elevation factor (more sophisticated curve)
    elevation = geo_data['elevation']
    if elevation < 5:  # Very low lying
        elevation_factor = 0.9
    elif elevation < 10:  # Low lying
        elevation_factor = 0.7
    elif elevation < 20:  # Moderate elevation
        elevation_factor = 0.4
    elif elevation < 35:  # Higher elevation
        elevation_factor = 0.2
    else:  # High elevation
        elevation_factor = 0.1
    
    # 2. River proximity factor (exponential decay)
    river_distance = geo_data['distance_to_major_river']
    if river_distance == 0:  # Directly on river
        river_factor = 1.0
    else:
        river_factor = max(0.1, np.exp(-river_distance / 10))  # Exponential decay
    
    # 3. Drainage quality factor (improved weighting)
    drainage_factors = {
        'Poor': 0.85,    # Very high risk
        'Moderate': 0.45, # Medium risk
        'Good': 0.15     # Low risk
    }
    drainage_factor = drainage_factors.get(geo_data['drainage_quality'], 0.45)
    
    # 4. Historical flood frequency factor
    flood_frequency = geo_data.get('flood_history_frequency', 5)
    frequency_factor = min(0.9, flood_frequency / 15.0)  # Normalize to 0-0.9
    
    # 5. Urbanization factor (affects runoff and drainage)
    urbanization = geo_data.get('urbanization_factor', 0.5)
    urban_factor = urbanization * 0.3  # Urban areas increase risk
    
    # 6. Rainfall climatology factor
    annual_rainfall = geo_data.get('annual_rainfall_mm', 2000)
    # Normalize to Bangladesh's range (roughly 1200-4000mm)
    rainfall_factor = min(0.8, (annual_rainfall - 1200) / 2800)
    rainfall_factor = max(0.1, rainfall_factor)
    
    # 7. Soil type factor
    soil_factors = {
        'clay_alluvial': 0.8,    # Poor drainage
        'sandy_clay': 0.6,       # Moderate drainage
        'silty_alluvial': 0.5,   # Moderate drainage
        'sandy_alluvial': 0.3,   # Better drainage
        'loamy': 0.2             # Good drainage
    }
    soil_factor = soil_factors.get(geo_data.get('soil_type', 'silty_alluvial'), 0.5)
    
    # 8. River confluence proximity (multiple rivers increase risk)
    confluence_distance = geo_data.get('river_confluence_distance', 50)
    confluence_factor = max(0.1, 1.0 - (confluence_distance / 100))
    
    # Ultra-conservative weighted combination
    geographic_risk = (
        elevation_factor * 0.12 +      # Further reduced elevation weight
        river_factor * 0.08 +          # Even lower river proximity weight
        drainage_factor * 0.06 +       # Even lower drainage weight
        frequency_factor * 0.06 +      # Even lower historical weight
        urban_factor * 0.04 +          # Even lower urban effects
        rainfall_factor * 0.03 +       # Even lower climate factor
        soil_factor * 0.02 +           # Even lower soil factor
        confluence_factor * 0.01       # Minimal confluence factor
    ) * 0.5  # Apply ultra-conservative multiplier
    
    # Ultra-conservative location-specific adjustments
    if location == 'Sylhet':  # Known for extreme flooding
        geographic_risk = min(0.25, geographic_risk * 1.005)  # Tiny boost
    elif location == 'Dhaka':  # Urban flooding issues
        geographic_risk = min(0.22, geographic_risk * 1.005)  # Tiny boost
    elif location == 'Rangpur':  # Generally safer
        geographic_risk = max(0.02, geographic_risk * 0.99)  # Tiny reduction
    
    return np.clip(geographic_risk, 0.02, 0.25)  # Ultra-low maximum cap

def calculate_fallback_risk(location, latest_rainfall, rainfall_3day, latest_water_level, threshold, geographic_risk):
    """Calculate conservative fallback risk when ML model is unavailable"""
    
    # Extremely conservative weather factors
    # Recent rainfall factor (very conservative response)
    if latest_rainfall > 30:  # Extreme rain (very rare)
        rain_factor = 0.45
    elif latest_rainfall > 20:  # Very heavy rain
        rain_factor = 0.35
    elif latest_rainfall > 12:   # Heavy rain
        rain_factor = 0.25
    elif latest_rainfall > 6:   # Moderate rain
        rain_factor = 0.15
    else:  # Light or no rain
        rain_factor = max(0.03, latest_rainfall / 25.0)
    
    # 3-day cumulative rainfall factor (very conservative thresholds)
    if rainfall_3day > 100:  # Extreme rainfall (extremely rare)
        cumulative_factor = 0.5
    elif rainfall_3day > 70:  # Very high rainfall
        cumulative_factor = 0.4
    elif rainfall_3day > 45:  # High rainfall
        cumulative_factor = 0.3
    elif rainfall_3day > 25:  # Moderate rainfall
        cumulative_factor = 0.18
    else:  # Normal to low rainfall
        cumulative_factor = max(0.03, rainfall_3day / 80.0)
    
    # Water level factor (very conservative thresholds)
    water_ratio = latest_water_level / threshold
    if water_ratio > 0.95:  # Very close to threshold
        water_factor = 0.5
    elif water_ratio > 0.85:  # Close to threshold
        water_factor = 0.4
    elif water_ratio > 0.75:  # Moderately high
        water_factor = 0.3
    elif water_ratio > 0.65:  # Somewhat elevated
        water_factor = 0.18
    else:  # Below concerning levels
        water_factor = max(0.03, water_ratio * 0.25)
    
    # Very conservative seasonal adjustment
    current_month = datetime.now().month
    if 6 <= current_month <= 9:  # Monsoon season
        seasonal_factor = 1.05  # Minimal boost
    elif current_month in [5, 10]:  # Pre/post monsoon
        seasonal_factor = 1.02  # Tiny boost
    else:  # Dry season
        seasonal_factor = 0.98  # Tiny reduction
    
    # Combine factors with ultra-conservative weights
    # Use extremely low base contribution from geographic risk
    ultra_conservative_geo_risk = min(geographic_risk, 0.15)  # Even lower cap on geographic influence
    
    fallback_risk = (
        rain_factor * 0.20 +           # Even less weight on recent rain
        cumulative_factor * 0.18 +    # Even less weight on 3-day accumulation
        water_factor * 0.15 +         # Even less weight on water level
        ultra_conservative_geo_risk * 0.04 +  # Even less geographic influence
        0.03  # Ultra-small constant base risk for sunny days
    ) * seasonal_factor
    
    # Ultra-conservative location adjustments
    geo_data = GEOGRAPHIC_DATA.get(location, {})
    if geo_data.get('flood_history_frequency', 5) > 9:  # Only extremely flood-prone areas
        fallback_risk *= 1.01  # Even tinier adjustment
    
    # Ensure ultra-conservative bounds - sunny days should be 5-10% max
    return np.clip(fallback_risk, 0.02, 0.35)

def calculate_transition_zone_factor(lat, lon):
    """Calculate a factor to smooth transitions between locations"""
    import math
    
    # Define transition zones around each location (in degrees, roughly 10-20km)
    transition_radius = 0.15  # About 15-20km depending on latitude
    
    total_influence = 0
    weighted_factors = {}
    
    for loc_name, (loc_lat, loc_lon) in LOCATIONS.items():
        distance = math.sqrt((lat - loc_lat)**2 + (lon - loc_lon)**2)
        
        if distance <= transition_radius:
            # Calculate influence (stronger closer to center)
            influence = 1.0 - (distance / transition_radius)
            influence = influence ** 0.5  # Square root for smoother transition
            
            total_influence += influence
            weighted_factors[loc_name] = influence
    
    # If we're in transition zones, return blended factor
    if total_influence > 0:
        # Normalize influences
        for loc_name in weighted_factors:
            weighted_factors[loc_name] /= total_influence
        
        return weighted_factors
    
    return None

def get_enhanced_interpolated_risk_for_coordinates(lat, lon):
    """Enhanced interpolation for coordinates with better accuracy"""
    import math
    
    # Calculate distances to all known locations
    distances = {}
    for loc_name, (loc_lat, loc_lon) in LOCATIONS.items():
        distance = math.sqrt((lat - loc_lat)**2 + (lon - loc_lon)**2)
        distances[loc_name] = distance
    
    # Find closest location
    closest_location = min(distances.items(), key=lambda x: x[1])
    closest_name, closest_distance = closest_location
    
    # If very close to a known location (within 0.008 degrees ≈ 0.9km), use that location
    if closest_distance < 0.008:
        return closest_name
    
    # Enhanced inverse distance weighting with exponential decay
    weights = {}
    total_weight = 0
    
    for loc_name, distance in distances.items():
        if distance < 0.001:  # Extremely close
            return loc_name
        
        # Use exponential inverse distance weighting for better locality
        # Closer locations have exponentially more influence
        weight = np.exp(-distance * 8) / (distance ** 1.5 + 0.01)
        weights[loc_name] = weight
        total_weight += weight
    
    # Normalize weights
    for loc_name in weights:
        weights[loc_name] /= total_weight
    
    # Interpolate all geographic factors with enhanced accuracy
    interpolated_elevation = 0
    interpolated_river_dist = 0
    interpolated_base_risk = 0
    interpolated_drainage_score = 0
    interpolated_urbanization = 0
    interpolated_annual_rainfall = 0
    interpolated_flood_frequency = 0
    
    # Enhanced drainage quality to numeric mapping
    drainage_to_numeric = {'Poor': 0.85, 'Moderate': 0.45, 'Good': 0.15}
    
    for loc_name, weight in weights.items():
        geo_data = GEOGRAPHIC_DATA.get(loc_name, GEOGRAPHIC_DATA['Dhaka'])
        
        interpolated_elevation += geo_data['elevation'] * weight
        interpolated_river_dist += geo_data['distance_to_major_river'] * weight
        interpolated_base_risk += geo_data['base_risk_factor'] * weight
        interpolated_drainage_score += drainage_to_numeric[geo_data['drainage_quality']] * weight
        interpolated_urbanization += geo_data.get('urbanization_factor', 0.5) * weight
        interpolated_annual_rainfall += geo_data.get('annual_rainfall_mm', 2000) * weight
        interpolated_flood_frequency += geo_data.get('flood_history_frequency', 5) * weight
    
    # Convert drainage score back to quality with more precise thresholds
    if interpolated_drainage_score >= 0.70:
        drainage_quality = 'Poor'
    elif interpolated_drainage_score >= 0.30:
        drainage_quality = 'Moderate'
    else:
        drainage_quality = 'Good'
    
    # Enhanced distance-based smoothing
    max_distance = max(distances.values())
    smoothing_factor = min(0.4, max_distance / 3.0)  # More aggressive smoothing for distant locations
    
    # Calculate enhanced averages for smoothing
    avg_elevation = sum(geo['elevation'] for geo in GEOGRAPHIC_DATA.values()) / len(GEOGRAPHIC_DATA)
    avg_base_risk = sum(geo['base_risk_factor'] for geo in GEOGRAPHIC_DATA.values()) / len(GEOGRAPHIC_DATA)
    avg_urbanization = sum(geo.get('urbanization_factor', 0.5) for geo in GEOGRAPHIC_DATA.values()) / len(GEOGRAPHIC_DATA)
    
    # Apply enhanced smoothing
    final_elevation = interpolated_elevation * (1 - smoothing_factor) + avg_elevation * smoothing_factor
    final_base_risk = interpolated_base_risk * (1 - smoothing_factor) + avg_base_risk * smoothing_factor
    final_urbanization = interpolated_urbanization * (1 - smoothing_factor) + avg_urbanization * smoothing_factor
    
    # Create enhanced interpolated geographic data
    return {
        'location_name': f"Enhanced Interpolated ({lat:.3f}, {lon:.3f})",
        'geographic_data': {
            'elevation': final_elevation,
            'distance_to_major_river': interpolated_river_dist,
            'drainage_quality': drainage_quality,
            'base_risk_factor': final_base_risk,
            'urbanization_factor': final_urbanization,
            'annual_rainfall_mm': interpolated_annual_rainfall,
            'flood_history_frequency': interpolated_flood_frequency,
            'interpolated': True,
            'primary_influence': max(weights.items(), key=lambda x: x[1])[0],
            'smoothing_applied': smoothing_factor,
            'weight_distribution': {loc: f"{weight:.3f}" for loc, weight in sorted(weights.items(), key=lambda x: x[1], reverse=True)[:3]}
        }
    }

def get_interpolated_weather_data(lat, lon, days=7):
    """Get weather data interpolated from nearby locations"""
    # Calculate distances and weights for weather interpolation
    distances = {}
    for loc_name, (loc_lat, loc_lon) in LOCATIONS.items():
        distance = ((lat - loc_lat)**2 + (lon - loc_lon)**2)**0.5
        distances[loc_name] = distance
    
    # Get weather data from multiple nearby locations
    weather_datasets = {}
    weights = {}
    total_weight = 0
    
    for loc_name, distance in distances.items():
        if distance < 2.0:  # Only use locations within reasonable distance
            try:
                weather_data = fetch_real_weather_data(loc_name, days)
                weather_datasets[loc_name] = weather_data
                
                # Weight by inverse distance squared
                weight = 1.0 / (distance + 0.1) ** 2
                weights[loc_name] = weight
                total_weight += weight
            except Exception as e:
                print(f"Could not fetch weather for {loc_name}: {e}")
                continue
    
    # If no weather data available, use nearest location
    if not weather_datasets:
        nearest_location = min(distances.items(), key=lambda x: x[1])[0]
        return fetch_real_weather_data(nearest_location, days)
    
    # Normalize weights
    for loc_name in weights:
        weights[loc_name] /= total_weight
    
    # Create interpolated weather data
    dates = weather_datasets[list(weather_datasets.keys())[0]]['date'].tolist()
    interpolated_rainfall = []
    
    for i in range(len(dates)):
        daily_rainfall = 0
        for loc_name, weight in weights.items():
            if loc_name in weather_datasets and i < len(weather_datasets[loc_name]):
                daily_rainfall += weather_datasets[loc_name]['rainfall'].iloc[i] * weight
        
        # Add small spatial variation based on distance from locations
        spatial_variation = 1.0 + (np.random.random() - 0.5) * 0.15  # ±7.5% variation
        daily_rainfall *= spatial_variation
        interpolated_rainfall.append(max(0, daily_rainfall))
    
    return pd.DataFrame({
        'date': dates,
        'rainfall': interpolated_rainfall
    })

def calculate_enhanced_transition_zone_factor(lat, lon):
    """Calculate enhanced transition zone factors with better accuracy"""
    import math
    
    # Enhanced transition zones with different radii based on geographic features
    base_transition_radius = 0.12  # About 12-15km
    
    total_influence = 0
    weighted_factors = {}
    
    for loc_name, (loc_lat, loc_lon) in LOCATIONS.items():
        distance = math.sqrt((lat - loc_lat)**2 + (lon - loc_lon)**2)
        
        # Dynamic transition radius based on location characteristics
        geo_data = GEOGRAPHIC_DATA.get(loc_name, {})
        
        # Locations near rivers have wider influence zones
        river_distance = geo_data.get('distance_to_major_river', 5)
        if river_distance < 2:  # Close to major river
            transition_radius = base_transition_radius * 1.3
        elif river_distance < 10:  # Moderate distance to river
            transition_radius = base_transition_radius * 1.1
        else:  # Far from rivers
            transition_radius = base_transition_radius * 0.9
        
        # Urban areas have more concentrated influence
        urbanization = geo_data.get('urbanization_factor', 0.5)
        if urbanization > 0.8:  # Highly urban
            transition_radius *= 0.85
        elif urbanization < 0.4:  # Rural
            transition_radius *= 1.15
        
        if distance <= transition_radius:
            # Enhanced influence calculation with smoother decay
            normalized_distance = distance / transition_radius
            
            # Use a smooth curve that gives more weight to closer locations
            # but still maintains reasonable influence for moderate distances
            influence = (1.0 - normalized_distance) ** 1.8
            influence = max(0.05, influence)  # Minimum influence
            
            total_influence += influence
            weighted_factors[loc_name] = influence
    
    # Enhanced normalization and filtering
    if total_influence > 0:
        # Normalize influences
        for loc_name in weighted_factors:
            weighted_factors[loc_name] /= total_influence
        
        # Filter out very small influences (less than 5%)
        filtered_factors = {
            loc: weight for loc, weight in weighted_factors.items() 
            if weight >= 0.05
        }
        
        # Re-normalize after filtering
        if filtered_factors:
            total_filtered = sum(filtered_factors.values())
            for loc_name in filtered_factors:
                filtered_factors[loc_name] /= total_filtered
            
            return filtered_factors
    
    return None
@app.route('/api/predict/coordinates/<float:lat>/<float:lon>')
def predict_coordinates(lat, lon):
    """Get highly accurate flood prediction for arbitrary coordinates using advanced interpolation"""
    try:
        # Validate coordinates are within Bangladesh bounds
        if not (20.5 <= lat <= 26.7 and 88.0 <= lon <= 92.8):
            return jsonify({
                'error': 'Coordinates outside Bangladesh boundaries',
                'coordinates': {'lat': lat, 'lon': lon}
            }), 400
        
        # Get interpolated data with enhanced accuracy
        interpolated_data = get_enhanced_interpolated_risk_for_coordinates(lat, lon)
        
        if isinstance(interpolated_data, str):
            # Very close to a known location, use that location's prediction
            return predict_location(interpolated_data)
        
        # Generate weather data based on weighted average from nearby locations
        weather_data = get_interpolated_weather_data(lat, lon)
        
        # Use interpolated geographic data
        geo_data = interpolated_data['geographic_data']
        
        # Calculate enhanced water levels using interpolated factors
        latest_rainfall = weather_data['rainfall'].iloc[-1]
        rainfall_3day = weather_data['rainfall'].tail(3).sum()
        rainfall_7day = weather_data['rainfall'].sum()
        
        # Enhanced water level calculation
        elevation = geo_data['elevation']
        drainage_quality = geo_data.get('drainage_quality', 'Moderate')
        distance_to_river = geo_data['distance_to_major_river']
        urbanization = geo_data.get('urbanization_factor', 0.5)
        
        # Realistic water level modeling
        elevation_factor = max(0.3, 1.0 - (elevation / 50.0))
        drainage_multiplier = {'Poor': 1.4, 'Moderate': 1.1, 'Good': 0.8}.get(drainage_quality, 1.1)
        river_proximity_factor = max(0.5, 1.0 - (distance_to_river / 20.0))
        urban_runoff_factor = 1.0 + (urbanization * 0.3)
        
        base_water_level = (2.8 + elevation_factor * 2.2 + river_proximity_factor * 0.8)
        estimated_water_level = (base_water_level + 
                               (latest_rainfall * 0.08 * drainage_multiplier * urban_runoff_factor) +
                               (rainfall_3day * 0.04 * drainage_multiplier) +
                               np.random.normal(0, 0.12))
        estimated_water_level = max(estimated_water_level, 1.8)
        
        # Check if we're in a transition zone for smoother blending
        transition_factors = calculate_enhanced_transition_zone_factor(lat, lon)
        
        # Initialize variables for blending calculations
        blended_risk = 0
        total_weight = 0
        confidence_sum = 0
        smoothing_factor = 0.0
        
        if transition_factors and len(transition_factors) > 1:
            # We're in a transition zone - use advanced blending
            
            for loc_name, weight in (transition_factors.items() if transition_factors else []):
                try:
                    # Get prediction for this location with error handling
                    loc_response = predict_location(loc_name)
                    if hasattr(loc_response, 'get_json'):
                        loc_data = loc_response.get_json()
                        loc_risk = loc_data.get('risk_probability', 0.5)
                        loc_confidence = loc_data.get('confidence', 0.7)
                        
                        blended_risk += loc_risk * weight * loc_confidence
                        total_weight += weight * loc_confidence
                        confidence_sum += loc_confidence * weight
                except Exception as e:
                    print(f"Error getting data for {loc_name}: {e}")
                    # Use fallback calculation for this location
                    loc_geo = GEOGRAPHIC_DATA.get(loc_name, {})
                    loc_base_risk = loc_geo.get('base_risk_factor', 0.5)
                    fallback_risk = calculate_fallback_risk(
                        loc_name, latest_rainfall, rainfall_3day, 
                        estimated_water_level, 5.5, loc_base_risk
                    )
                    blended_risk += fallback_risk * weight * 0.6  # Lower confidence for fallback
                    total_weight += weight * 0.6
                    confidence_sum += 0.6 * weight
            
            if total_weight > 0:
                final_risk_score = blended_risk / total_weight
                prediction_confidence = min(0.95, confidence_sum / sum(transition_factors.values()))
                
                # Apply spatial smoothing to prevent sharp transitions
                smoothing_factor = 0.15 * (len(transition_factors) if transition_factors else 0)  # More smoothing with more influences
                geographic_base = geo_data.get('base_risk_factor', 0.5)
                final_risk_score = (final_risk_score * (1 - smoothing_factor) + 
                                  geographic_base * smoothing_factor)
                
                final_risk_score = np.clip(final_risk_score, 0.05, 0.95)
                
                # Generate comprehensive flood risk profile for coordinates
                coordinate_flood_profile = calculate_flood_risk_profile(None, lat=lat, lon=lon)
                
                # Override final_risk_score with weighted overall risk from flood types
                final_risk_score = calculate_weighted_overall_risk(coordinate_flood_profile)
                
                # Enhanced status determination
                if final_risk_score >= 0.85:
                    status = 'EXTREME RISK'
                    risk_class = 'risk-extreme'
                elif final_risk_score >= 0.75:
                    status = 'CRITICAL RISK'
                    risk_class = 'risk-critical'
                elif final_risk_score >= 0.6:
                    status = 'HIGH RISK'
                    risk_class = 'risk-high'
                elif final_risk_score >= 0.4:
                    status = 'MODERATE RISK'
                    risk_class = 'risk-medium'
                else:
                    status = 'LOW RISK'
                    risk_class = 'risk-low'
                
                response_data = {
                    'location': f"Coordinates ({lat:.3f}, {lon:.3f})",
                    'coordinates': {'lat': lat, 'lon': lon},
                    'timestamp': datetime.now().isoformat(),
                    'current_rainfall': float(latest_rainfall),
                    'current_water_level': float(estimated_water_level),
                    'flood_threshold': 5.5,
                    'flood_risk': int(final_risk_score > 0.6),
                    'risk_probability': float(final_risk_score),
                    'confidence': float(prediction_confidence),
                    'status': status,
                    'risk_class': risk_class,
                    'geographic_factors': {
                        'elevation_m': geo_data['elevation'],
                        'distance_to_river_km': geo_data['distance_to_major_river'],
                        'drainage_quality': geo_data.get('drainage_quality', 'Unknown'),
                        'urbanization_factor': geo_data.get('urbanization_factor', 0.5),
                        'interpolated': True,
                        'in_transition_zone': True,
                        'base_risk_factor': float(geo_data.get('base_risk_factor', 0.5)),
                        'transition_influences': {loc: f"{weight:.3f}" for loc, weight in (transition_factors.items() if transition_factors else [])},
                        'smoothing_applied': smoothing_factor
                    },
                    'flood_risk_profile': {
                        flood_type: {
                            'risk_percentage': float(profile['risk_percentage']),
                            'severity_level': profile['severity_level'],
                            'flood_degree': profile['flood_degree'],
                            'estimated_depth': profile['estimated_depth'],
                            'description': profile['description'],
                            'typical_damage': profile['typical_damage']
                        } for flood_type, profile in (coordinate_flood_profile.items() if coordinate_flood_profile else {})
                    },
                    'model_info': {
                        'version': '2.0.0',
                        'prediction_method': 'enhanced_transition_blend',
                        'locations_used': len(transition_factors) if transition_factors else 0,
                        'total_weight': float(total_weight)
                    },
                    'note': f'Advanced prediction in transition zone (blended from {len(transition_factors) if transition_factors else 0} locations)'
                }
                
                return jsonify(response_data)
        
        # Not in transition zone or single influence - use enhanced interpolation
        geographic_risk = geo_data.get('base_risk_factor', 0.5)
        smoothing_factor = geo_data.get('smoothing_applied', 0.0)  # Initialize smoothing factor
        
        # Try to use enhanced ML model if available and we have enough data
        if rf_model is not None and scaler is not None and len(feature_cols) == 9:
            try:
                # Water level trend calculation
                water_level_trend = 0  # Default for coordinates (no historical data)
                current_month = datetime.now().month
                is_monsoon = 1 if 6 <= current_month <= 9 else 0
                
                # Create feature array matching training data
                features = np.array([[
                    latest_rainfall, rainfall_3day, rainfall_7day,
                    estimated_water_level, water_level_trend, is_monsoon,
                    elevation, distance_to_river, geographic_risk
                ]])
                
                # Get ML prediction
                features_scaled = scaler.transform(features)
                ml_risk_probability = rf_model.predict_proba(features_scaled)[0, 1]
                
                # Adjust confidence based on interpolation uncertainty
                interpolation_uncertainty = geo_data.get('smoothing_applied', 0) * 0.3
                model_confidence = max(0.6, 0.9 - interpolation_uncertainty)
                
                # Enhanced risk calculation with geographic validation
                final_risk_score = (
                    ml_risk_probability * 0.7 * model_confidence +
                    geographic_risk * 0.3
                )
                
                # Apply stability constraints for interpolated locations
                primary_influence = geo_data.get('primary_influence')
                if primary_influence and primary_influence in LOCATIONS:
                    try:
                        primary_response = predict_location(primary_influence)
                        if hasattr(primary_response, 'get_json'):
                            primary_data = primary_response.get_json()
                            primary_risk = primary_data.get('risk_probability', 0.5)
                            
                            # Limit deviation from primary location (more restrictive for interpolated)
                            max_deviation = 0.15  # 15% max deviation
                            if abs(final_risk_score - primary_risk) > max_deviation:
                                if final_risk_score > primary_risk:
                                    final_risk_score = primary_risk + max_deviation
                                else:
                                    final_risk_score = primary_risk - max_deviation
                    except:
                        pass  # Continue with ML prediction if primary location fails
                
                prediction_confidence = model_confidence
                prediction_method = 'enhanced_ml_interpolation'
                
            except Exception as e:
                print(f"ML prediction error for coordinates: {e}")
                # Fallback to enhanced geographic calculation
                final_risk_score = calculate_fallback_risk(
                    f"coords_{lat}_{lon}", latest_rainfall, rainfall_3day,
                    estimated_water_level, 5.5, geographic_risk
                )
                prediction_confidence = 0.7
                prediction_method = 'enhanced_fallback'
        else:
            # Enhanced fallback calculation
            final_risk_score = calculate_fallback_risk(
                f"coords_{lat}_{lon}", latest_rainfall, rainfall_3day,
                estimated_water_level, 5.5, geographic_risk
            )
            prediction_confidence = 0.7
            prediction_method = 'enhanced_fallback'
        
        final_risk_score = np.clip(final_risk_score, 0.05, 0.95)
        
        # Enhanced status determination
        if final_risk_score >= 0.85:
            status = 'EXTREME RISK'
            risk_class = 'risk-extreme'
        elif final_risk_score >= 0.75:
            status = 'CRITICAL RISK'
            risk_class = 'risk-critical'
        elif final_risk_score >= 0.6:
            status = 'HIGH RISK'
            risk_class = 'risk-high'
        elif final_risk_score >= 0.4:
            status = 'MODERATE RISK'
            risk_class = 'risk-medium'
        else:
            status = 'LOW RISK'
            risk_class = 'risk-low'
        
        # Generate comprehensive flood risk profile for coordinates
        coordinate_flood_profile = calculate_flood_risk_profile(None, lat=lat, lon=lon)
        
        # Override final_risk_score with weighted overall risk from flood types
        final_risk_score = calculate_weighted_overall_risk(coordinate_flood_profile)
        
        # Recalculate status based on weighted risk
        if final_risk_score >= 0.85:
            status = 'EXTREME RISK'
            risk_class = 'risk-extreme'
        elif final_risk_score >= 0.75:
            status = 'CRITICAL RISK'  
            risk_class = 'risk-critical'
        elif final_risk_score >= 0.6:
            status = 'HIGH RISK'
            risk_class = 'risk-high'
        elif final_risk_score >= 0.4:
            status = 'MODERATE RISK'
            risk_class = 'risk-medium'
        elif final_risk_score >= 0.2:
            status = 'LOW RISK'
            risk_class = 'risk-low'
        else:
            status = 'MINIMAL RISK'
            risk_class = 'risk-minimal'
        
        response_data = {
            'location': f"Coordinates ({lat:.3f}, {lon:.3f})",
            'coordinates': {'lat': lat, 'lon': lon},
            'timestamp': datetime.now().isoformat(),
            'current_rainfall': float(latest_rainfall),
            'current_water_level': float(estimated_water_level),
            'flood_threshold': 5.5,
            'flood_risk': int(final_risk_score > 0.6),
            'risk_probability': float(final_risk_score),
            'confidence': float(prediction_confidence),
            'status': status,
            'risk_class': risk_class,
            'geographic_factors': {
                'elevation_m': geo_data['elevation'],
                'distance_to_river_km': geo_data['distance_to_major_river'],
                'drainage_quality': geo_data.get('drainage_quality', 'Unknown'),
                'urbanization_factor': geo_data.get('urbanization_factor', 0.5),
                'interpolated': True,
                'in_transition_zone': True,
                'base_risk_factor': float(geo_data.get('base_risk_factor', 0.5)),
                'transition_influences': {loc: f"{weight:.3f}" for loc, weight in (transition_factors.items() if transition_factors else [])},
                'smoothing_applied': smoothing_factor
            },
            'flood_risk_profile': {
                flood_type: {
                    'risk_percentage': float(profile['risk_percentage']),
                    'severity_level': profile['severity_level'],
                    'flood_degree': profile['flood_degree'],
                    'estimated_depth': profile['estimated_depth'],
                    'description': profile['description'],
                    'typical_damage': profile['typical_damage']
                } for flood_type, profile in (coordinate_flood_profile.items() if coordinate_flood_profile else {})
            },
            'model_info': {
                'version': '2.0.0',
                'prediction_method': 'enhanced_transition_blend',
                'locations_used': len(transition_factors) if transition_factors else 0,
                'total_weight': float(total_weight)
            },
            'note': f'Advanced prediction in transition zone (blended from {len(transition_factors) if transition_factors else 0} locations)'
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        print(f"Coordinate prediction error for ({lat}, {lon}): {str(e)}")
        return jsonify({
            'error': f'Prediction failed: {str(e)}',
            'coordinates': {'lat': lat, 'lon': lon}
        }), 500

def calculate_weighted_overall_risk(flood_risk_profile):
    """Calculate overall risk as weighted average of individual flood type risks"""
    
    # Safety check for None input
    if not flood_risk_profile:
        return 0.05  # Default minimal risk
    
    # Define weights for each flood type (should sum to 1.0)
    weights = {
        'riverine': 0.35,        # Highest weight - major infrastructure impact
        'urban_drainage': 0.30,  # High weight - affects daily life significantly  
        'flash': 0.25,          # Moderate weight - immediate danger but localized
        'tidal': 0.10           # Lower weight - limited to coastal areas
    }
    
    weighted_sum = 0.0
    total_weight = 0.0
    
    for flood_type, profile in flood_risk_profile.items():
        if flood_type in weights and profile:
            risk_percentage = profile.get('risk_percentage', 0)
            weight = weights[flood_type]
            weighted_sum += (risk_percentage / 100.0) * weight
            total_weight += weight
    
    # Normalize by total weight (in case not all flood types are present)
    if total_weight > 0:
        overall_risk = weighted_sum / total_weight
    else:
        overall_risk = 0.05  # Default minimal risk
    
    return overall_risk

if __name__ == '__main__':
    import os
    port = int(os.environ.get('PORT', 10000))  # Render uses port 10000 by default
    debug = os.environ.get('FLASK_ENV') != 'production'
    app.run(debug=debug, host='0.0.0.0', port=port)
