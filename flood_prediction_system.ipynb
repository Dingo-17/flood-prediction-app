{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d417066",
   "metadata": {},
   "source": [
    "# üåä Bangladesh Flood Prediction System\n",
    "\n",
    "## üìã Project Overview\n",
    "\n",
    "This comprehensive system predicts river flooding in Bangladesh based on:\n",
    "- **Rainfall data** from the past N days\n",
    "- **River water levels** from monitoring stations\n",
    "- **Live API integration** for real-time predictions\n",
    "- **Alert system** via Telegram/Email\n",
    "- **CSV logging** for historical tracking\n",
    "\n",
    "### üéØ Objectives\n",
    "1. Build XGBoost and LSTM models for flood prediction\n",
    "2. Integrate live rainfall data from weather APIs\n",
    "3. Create automated alert system\n",
    "4. Log predictions for monitoring and analysis\n",
    "\n",
    "### üìä Data Sources\n",
    "- **Rainfall**: OpenWeatherMap API, MeteoSource API\n",
    "- **River Levels**: BWDB (Bangladesh Water Development Board)\n",
    "- **Historical Data**: Kaggle Bangladesh flood datasets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448c5de0",
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/digantohaque/python/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <98D50080-9632-3EA4-B874-146E55453763> /Users/digantohaque/python/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Machine Learning Libraries\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgb\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python/.venv/lib/python3.13/site-packages/xgboost/__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     Booster,\n\u001b[32m     10\u001b[39m     DataIter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     build_info,\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python/.venv/lib/python3.13/site-packages/xgboost/tracker.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, _deprecate_positional_args, make_jcargs\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python/.venv/lib/python3.13/site-packages/xgboost/core.py:295\u001b[39m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m _LIB = \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    299\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    300\u001b[39m \n\u001b[32m    301\u001b[39m \u001b[33;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m \u001b[33;03m        return value from API calls\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python/.venv/lib/python3.13/site-packages/xgboost/core.py:257\u001b[39m, in \u001b[36m_load_lib\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[32m    256\u001b[39m         libname = os.path.basename(lib_paths[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[32m    258\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[33mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) could not be loaded.\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[33mLikely causes:\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[33m  * OpenMP runtime is not installed\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[33m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[33m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[33m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[33m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[32m    266\u001b[39m \n\u001b[32m    267\u001b[39m \u001b[33m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[32m    268\u001b[39m \n\u001b[32m    269\u001b[39m \u001b[33mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    271\u001b[39m         )\n\u001b[32m    272\u001b[39m     _register_log_callback(lib)\n\u001b[32m    274\u001b[39m     libver = _lib_version(lib)\n",
      "\u001b[31mXGBoostError\u001b[39m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/digantohaque/python/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <98D50080-9632-3EA4-B874-146E55453763> /Users/digantohaque/python/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries and Set Up Environment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# API and Web Libraries\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Alert and Logging Libraries\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üì¶ XGBoost version: {xgb.__version__}\")\n",
    "print(f\"üß† TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Create directories for logs and data\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('alerts', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55acbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and API Keys\n",
    "# ‚ö†Ô∏è IMPORTANT: Replace with your actual API keys\n",
    "\n",
    "# OpenWeatherMap API Configuration\n",
    "OPENWEATHER_API_KEY = \"YOUR_OPENWEATHER_API_KEY\"\n",
    "OPENWEATHER_BASE_URL = \"https://api.openweathermap.org/data/2.5/onecall/timemachine\"\n",
    "\n",
    "# MeteoSource API Configuration (Alternative)\n",
    "METEOSOURCE_API_KEY = \"YOUR_METEOSOURCE_API_KEY\"\n",
    "METEOSOURCE_BASE_URL = \"https://www.meteosource.com/api/v1/free/point\"\n",
    "\n",
    "# Bangladesh key locations (lat, lon)\n",
    "LOCATIONS = {\n",
    "    'Dhaka': (23.8103, 90.4125),\n",
    "    'Sylhet': (24.8949, 91.8687),\n",
    "    'Rangpur': (25.7439, 89.2752),\n",
    "    'Bahadurabad': (25.1906, 89.7006),  # Major river gauge station\n",
    "    'Chittagong': (22.3569, 91.7832)\n",
    "}\n",
    "\n",
    "# Flood thresholds (in meters) - Based on BWDB danger levels\n",
    "FLOOD_THRESHOLDS = {\n",
    "    'Dhaka': 5.5,\n",
    "    'Sylhet': 6.0,\n",
    "    'Rangpur': 4.8,\n",
    "    'Bahadurabad': 7.2,\n",
    "    'Chittagong': 3.5\n",
    "}\n",
    "\n",
    "# Alert Configuration\n",
    "TELEGRAM_BOT_TOKEN = \"YOUR_TELEGRAM_BOT_TOKEN\"\n",
    "TELEGRAM_CHAT_ID = \"YOUR_TELEGRAM_CHAT_ID\"\n",
    "\n",
    "EMAIL_CONFIG = {\n",
    "    'smtp_server': 'smtp.gmail.com',\n",
    "    'port': 587,\n",
    "    'sender_email': 'YOUR_EMAIL@gmail.com',\n",
    "    'sender_password': 'YOUR_APP_PASSWORD',\n",
    "    'recipient_email': 'ALERT_RECIPIENT@gmail.com'\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration loaded successfully!\")\n",
    "print(f\"üìç Monitoring {len(LOCATIONS)} locations in Bangladesh\")\n",
    "print(f\"üö® Flood thresholds configured for {len(FLOOD_THRESHOLDS)} stations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393f15f8",
   "metadata": {},
   "source": [
    "## üì• Data Acquisition and Loading\n",
    "\n",
    "### Dataset Sources:\n",
    "1. **Historical Rainfall Data**: Bangladesh meteorological data\n",
    "2. **River Water Levels**: BWDB monitoring stations\n",
    "3. **Flood Records**: Historical flood events (2019-2024)\n",
    "\n",
    "You can download datasets from:\n",
    "- **Kaggle**: \"Regression-Based Flood Prediction in Bangladesh\"\n",
    "- **BWDB**: Bangladesh Water Development Board\n",
    "- **BMD**: Bangladesh Meteorological Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Synthetic Historical Data for Demonstration\n",
    "# In production, replace this with actual data loading\n",
    "\n",
    "def generate_synthetic_data(days=1095):  # 3 years of data\n",
    "    \"\"\"Generate synthetic rainfall and river level data for Bangladesh\"\"\"\n",
    "    \n",
    "    # Date range\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=days)\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Simulate seasonal patterns (monsoon season: June-September)\n",
    "    seasonal_factor = np.array([\n",
    "        1.5 if 6 <= date.month <= 9 else 0.8 \n",
    "        for date in dates\n",
    "    ])\n",
    "    \n",
    "    # Generate rainfall data (mm/day)\n",
    "    base_rainfall = np.random.gamma(2, 2, len(dates))  # Gamma distribution for rainfall\n",
    "    rainfall = base_rainfall * seasonal_factor + np.random.normal(0, 2, len(dates))\n",
    "    rainfall = np.maximum(rainfall, 0)  # No negative rainfall\n",
    "    \n",
    "    # Generate river water level (meters)\n",
    "    # Water level correlates with rainfall with some lag\n",
    "    base_level = 3.5  # Base water level\n",
    "    level_response = np.convolve(rainfall, [0.1, 0.2, 0.3, 0.2, 0.1, 0.05, 0.05], mode='same')\n",
    "    water_level = base_level + level_response + np.random.normal(0, 0.3, len(dates))\n",
    "    water_level = np.maximum(water_level, 1.0)  # Minimum water level\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'rainfall': rainfall,\n",
    "        'water_level': water_level,\n",
    "        'month': [d.month for d in dates],\n",
    "        'year': [d.year for d in dates],\n",
    "        'season': ['monsoon' if 6 <= d.month <= 9 else 'dry' for d in dates]\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate synthetic data\n",
    "print(\"üîÑ Generating synthetic historical data...\")\n",
    "historical_data = generate_synthetic_data(days=1095)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"üìä Dataset shape: {historical_data.shape}\")\n",
    "print(f\"üìÖ Date range: {historical_data['date'].min()} to {historical_data['date'].max()}\")\n",
    "print(\"\\nüìã First 5 rows:\")\n",
    "print(historical_data.head())\n",
    "\n",
    "print(\"\\nüìà Statistical Summary:\")\n",
    "print(historical_data[['rainfall', 'water_level']].describe())\n",
    "\n",
    "# Save to CSV for future use\n",
    "historical_data.to_csv('data/historical_flood_data.csv', index=False)\n",
    "print(\"üíæ Data saved to 'data/historical_flood_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb01fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Rainfall time series\n",
    "axes[0,0].plot(historical_data['date'], historical_data['rainfall'], alpha=0.7, color='blue')\n",
    "axes[0,0].set_title('Daily Rainfall (mm)', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Rainfall (mm)')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Water level time series\n",
    "axes[0,1].plot(historical_data['date'], historical_data['water_level'], alpha=0.7, color='green')\n",
    "axes[0,1].axhline(y=5.5, color='red', linestyle='--', label='Flood Threshold (5.5m)')\n",
    "axes[0,1].set_title('River Water Level (m)', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_ylabel('Water Level (m)')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Monthly rainfall distribution\n",
    "monthly_rain = historical_data.groupby('month')['rainfall'].mean()\n",
    "axes[1,0].bar(monthly_rain.index, monthly_rain.values, color='skyblue', alpha=0.8)\n",
    "axes[1,0].set_title('Average Monthly Rainfall', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Month')\n",
    "axes[1,0].set_ylabel('Average Rainfall (mm)')\n",
    "axes[1,0].set_xticks(range(1, 13))\n",
    "\n",
    "# Correlation between rainfall and water level\n",
    "axes[1,1].scatter(historical_data['rainfall'], historical_data['water_level'], \n",
    "                  alpha=0.5, color='purple')\n",
    "axes[1,1].set_title('Rainfall vs Water Level Correlation', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Rainfall (mm)')\n",
    "axes[1,1].set_ylabel('Water Level (m)')\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = historical_data['rainfall'].corr(historical_data['water_level'])\n",
    "axes[1,1].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "               transform=axes[1,1].transAxes, fontsize=12, \n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Rainfall-Water Level Correlation: {correlation:.3f}\")\n",
    "print(f\"üåßÔ∏è Average Daily Rainfall: {historical_data['rainfall'].mean():.2f} mm\")\n",
    "print(f\"üåä Average Water Level: {historical_data['water_level'].mean():.2f} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a1b2d1",
   "metadata": {},
   "source": [
    "## üîß Data Preprocessing and Feature Engineering\n",
    "\n",
    "This section creates the features needed for flood prediction:\n",
    "1. **Flood Labels**: Binary classification (flood/no flood) based on water level threshold\n",
    "2. **Lag Features**: Rainfall and water level from previous N days\n",
    "3. **Rolling Statistics**: Moving averages and cumulative rainfall\n",
    "4. **Seasonal Features**: Month, season indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Function\n",
    "def create_flood_features(df, flood_threshold=5.5, lag_days=7):\n",
    "    \"\"\"\n",
    "    Create features for flood prediction model\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with 'date', 'rainfall', 'water_level' columns\n",
    "    - flood_threshold: Water level threshold for flood classification (meters)\n",
    "    - lag_days: Number of previous days to include as features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # 1. Create flood label (binary target)\n",
    "    df_features['flood'] = (df_features['water_level'] > flood_threshold).astype(int)\n",
    "    \n",
    "    # 2. Create lag features for rainfall\n",
    "    for i in range(1, lag_days + 1):\n",
    "        df_features[f'rainfall_lag{i}'] = df_features['rainfall'].shift(i)\n",
    "    \n",
    "    # 3. Create lag features for water level\n",
    "    for i in range(1, lag_days + 1):\n",
    "        df_features[f'water_level_lag{i}'] = df_features['water_level'].shift(i)\n",
    "    \n",
    "    # 4. Rolling statistics\n",
    "    df_features['rainfall_3day_avg'] = df_features['rainfall'].rolling(window=3).mean()\n",
    "    df_features['rainfall_7day_avg'] = df_features['rainfall'].rolling(window=7).mean()\n",
    "    df_features['rainfall_3day_sum'] = df_features['rainfall'].rolling(window=3).sum()\n",
    "    df_features['rainfall_7day_sum'] = df_features['rainfall'].rolling(window=7).sum()\n",
    "    \n",
    "    # 5. Water level statistics\n",
    "    df_features['water_level_3day_avg'] = df_features['water_level'].rolling(window=3).mean()\n",
    "    df_features['water_level_7day_max'] = df_features['water_level'].rolling(window=7).max()\n",
    "    df_features['water_level_trend'] = df_features['water_level'] - df_features['water_level_lag1']\n",
    "    \n",
    "    # 6. Seasonal features\n",
    "    df_features['is_monsoon'] = df_features['season'].apply(lambda x: 1 if x == 'monsoon' else 0)\n",
    "    df_features['month_sin'] = np.sin(2 * np.pi * df_features['month'] / 12)\n",
    "    df_features['month_cos'] = np.cos(2 * np.pi * df_features['month'] / 12)\n",
    "    \n",
    "    # 7. Interaction features\n",
    "    df_features['rain_water_interaction'] = df_features['rainfall'] * df_features['water_level_lag1']\n",
    "    \n",
    "    # Drop rows with NaN values (due to lag features)\n",
    "    df_features = df_features.dropna().reset_index(drop=True)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"üîß Creating features for flood prediction...\")\n",
    "flood_threshold = 5.5  # meters\n",
    "lag_days = 7\n",
    "\n",
    "flood_data = create_flood_features(historical_data, flood_threshold, lag_days)\n",
    "\n",
    "print(f\"üìä Features created! Dataset shape: {flood_data.shape}\")\n",
    "print(f\"üéØ Flood events: {flood_data['flood'].sum()} out of {len(flood_data)} days ({flood_data['flood'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Display feature columns\n",
    "feature_columns = [col for col in flood_data.columns if col not in ['date', 'rainfall', 'water_level', 'month', 'year', 'season']]\n",
    "print(f\"\\nüìã Features created ({len(feature_columns)}):\")\n",
    "for i, col in enumerate(feature_columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# Show sample of engineered features\n",
    "print(\"\\nüìã Sample of engineered features:\")\n",
    "print(flood_data[['date', 'rainfall', 'water_level', 'flood', 'rainfall_lag1', 'rainfall_3day_sum', 'water_level_trend']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e375c3",
   "metadata": {},
   "source": [
    "## üìä Train-Test Split\n",
    "\n",
    "For time series data, we use chronological split to avoid data leakage:\n",
    "- **Training set**: First 80% of the data (chronologically)\n",
    "- **Test set**: Last 20% of the data\n",
    "- **Features**: All lag and engineered features\n",
    "- **Target**: Binary flood classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df8f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "# Select feature columns (exclude date and original columns)\n",
    "feature_cols = [col for col in flood_data.columns \n",
    "                if col not in ['date', 'rainfall', 'water_level', 'month', 'year', 'season', 'flood']]\n",
    "\n",
    "X = flood_data[feature_cols]\n",
    "y = flood_data['flood']\n",
    "\n",
    "print(f\"üéØ Features selected: {len(feature_cols)}\")\n",
    "print(f\"üìä Feature matrix shape: {X.shape}\")\n",
    "print(f\"üéØ Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Time series split (chronological)\n",
    "split_index = int(len(flood_data) * 0.8)\n",
    "\n",
    "X_train = X.iloc[:split_index]\n",
    "X_test = X.iloc[split_index:]\n",
    "y_train = y.iloc[:split_index]\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "# Get corresponding dates for evaluation\n",
    "train_dates = flood_data['date'].iloc[:split_index]\n",
    "test_dates = flood_data['date'].iloc[split_index:]\n",
    "\n",
    "print(f\"\\nüìà Training set:\")\n",
    "print(f\"   ‚Ä¢ Size: {len(X_train)} samples\")\n",
    "print(f\"   ‚Ä¢ Date range: {train_dates.min()} to {train_dates.max()}\")\n",
    "print(f\"   ‚Ä¢ Flood events: {y_train.sum()} ({y_train.mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüß™ Test set:\")\n",
    "print(f\"   ‚Ä¢ Size: {len(X_test)} samples\")\n",
    "print(f\"   ‚Ä¢ Date range: {test_dates.min()} to {test_dates.max()}\")\n",
    "print(f\"   ‚Ä¢ Flood events: {y_test.sum()} ({y_test.mean()*100:.1f}%)\")\n",
    "\n",
    "# Feature scaling (for LSTM later)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n‚úÖ Data split and scaling completed!\")\n",
    "print(f\"üìã Feature columns: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff51586a",
   "metadata": {},
   "source": [
    "## üöÄ Model Training: XGBoost\n",
    "\n",
    "XGBoost is excellent for tabular data and provides:\n",
    "- High accuracy on structured data\n",
    "- Feature importance rankings\n",
    "- Fast training and prediction\n",
    "- Handles missing values well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost Model\n",
    "print(\"üöÄ Training XGBoost model...\")\n",
    "\n",
    "# Configure XGBoost with optimal parameters\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ XGBoost training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb)\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb)\n",
    "auc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "\n",
    "print(f\"\\nüìä XGBoost Performance Metrics:\")\n",
    "print(f\"   ‚Ä¢ Accuracy:  {accuracy_xgb:.4f}\")\n",
    "print(f\"   ‚Ä¢ Precision: {precision_xgb:.4f}\")\n",
    "print(f\"   ‚Ä¢ Recall:    {recall_xgb:.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score:  {f1_xgb:.4f}\")\n",
    "print(f\"   ‚Ä¢ AUC-ROC:   {auc_xgb:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "print(f\"\\nüîç Confusion Matrix:\")\n",
    "print(f\"   True Negatives:  {cm_xgb[0,0]}\")\n",
    "print(f\"   False Positives: {cm_xgb[0,1]}\")\n",
    "print(f\"   False Negatives: {cm_xgb[1,0]}\")\n",
    "print(f\"   True Positives:  {cm_xgb[1,1]}\")\n",
    "\n",
    "# Save the model\n",
    "import joblib\n",
    "joblib.dump(xgb_model, 'models/xgboost_flood_model.pkl')\n",
    "joblib.dump(scaler, 'models/feature_scaler.pkl')\n",
    "print(\"\\nüíæ XGBoost model saved to 'models/xgboost_flood_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac845018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"üéØ Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='skyblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('XGBoost Feature Importance (Top 15)', fontsize=16, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(top_features['importance']):\n",
    "    plt.text(v + 0.001, i, f'{v:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('models/xgboost_feature_importance.csv', index=False)\n",
    "print(\"üíæ Feature importance saved to 'models/xgboost_feature_importance.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb407ec4",
   "metadata": {},
   "source": [
    "## üß† Model Training: LSTM\n",
    "\n",
    "LSTM (Long Short-Term Memory) networks are designed for sequence data and can:\n",
    "- Capture temporal dependencies\n",
    "- Learn complex patterns over time\n",
    "- Handle variable-length sequences\n",
    "- Remember important information across time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245059df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM (sequence format)\n",
    "def create_sequences(X, y, window_size=7):\n",
    "    \"\"\"Convert data to sequences for LSTM\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(window_size, len(X)):\n",
    "        X_seq.append(X[i-window_size:i])\n",
    "        y_seq.append(y[i])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Create sequences\n",
    "window_size = 7  # 7-day sequences\n",
    "print(f\"üîÑ Creating sequences with window size: {window_size}\")\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train.values, window_size)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test.values, window_size)\n",
    "\n",
    "print(f\"üìä LSTM Training sequences shape: {X_train_seq.shape}\")\n",
    "print(f\"üìä LSTM Test sequences shape: {X_test_seq.shape}\")\n",
    "\n",
    "# Build LSTM Model\n",
    "print(\"üß† Building LSTM model...\")\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(window_size, len(feature_cols))),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "lstm_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "print(\"üìã LSTM Model Architecture:\")\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train LSTM\n",
    "print(\"üöÄ Training LSTM model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    shuffle=False  # Important for time series\n",
    ")\n",
    "\n",
    "training_time_lstm = time.time() - start_time\n",
    "print(f\"‚úÖ LSTM training completed in {training_time_lstm:.2f} seconds\")\n",
    "\n",
    "# Save LSTM model\n",
    "lstm_model.save('models/lstm_flood_model.h5')\n",
    "print(\"üíæ LSTM model saved to 'models/lstm_flood_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Predictions and Evaluation\n",
    "y_pred_lstm_proba = lstm_model.predict(X_test_seq)\n",
    "y_pred_lstm = (y_pred_lstm_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate LSTM metrics\n",
    "accuracy_lstm = accuracy_score(y_test_seq, y_pred_lstm)\n",
    "precision_lstm = precision_score(y_test_seq, y_pred_lstm)\n",
    "recall_lstm = recall_score(y_test_seq, y_pred_lstm)\n",
    "f1_lstm = f1_score(y_test_seq, y_pred_lstm)\n",
    "auc_lstm = roc_auc_score(y_test_seq, y_pred_lstm_proba)\n",
    "\n",
    "print(f\"üìä LSTM Performance Metrics:\")\n",
    "print(f\"   ‚Ä¢ Accuracy:  {accuracy_lstm:.4f}\")\n",
    "print(f\"   ‚Ä¢ Precision: {precision_lstm:.4f}\")\n",
    "print(f\"   ‚Ä¢ Recall:    {recall_lstm:.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score:  {f1_lstm:.4f}\")\n",
    "print(f\"   ‚Ä¢ AUC-ROC:   {auc_lstm:.4f}\")\n",
    "\n",
    "# Model Comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'LSTM'],\n",
    "    'Accuracy': [accuracy_xgb, accuracy_lstm],\n",
    "    'Precision': [precision_xgb, precision_lstm],\n",
    "    'Recall': [recall_xgb, recall_lstm],\n",
    "    'F1-Score': [f1_xgb, f1_lstm],\n",
    "    'AUC-ROC': [auc_xgb, auc_lstm],\n",
    "    'Training Time (s)': [training_time, training_time_lstm]\n",
    "})\n",
    "\n",
    "print(f\"\\nüèÜ Model Comparison:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Plot training history for LSTM\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Accuracy\n",
    "axes[0,0].plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "axes[0,0].plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "axes[0,0].set_title('Model Accuracy', fontweight='bold')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[0,1].plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "axes[0,1].plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
    "axes[0,1].set_title('Model Loss', fontweight='bold')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('Loss')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "axes[1,0].plot(history.history['precision'], label='Training Precision', color='blue')\n",
    "axes[1,0].plot(history.history['val_precision'], label='Validation Precision', color='red')\n",
    "axes[1,0].set_title('Model Precision', fontweight='bold')\n",
    "axes[1,0].set_xlabel('Epoch')\n",
    "axes[1,0].set_ylabel('Precision')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1,1].plot(history.history['recall'], label='Training Recall', color='blue')\n",
    "axes[1,1].plot(history.history['val_recall'], label='Validation Recall', color='red')\n",
    "axes[1,1].set_title('Model Recall', fontweight='bold')\n",
    "axes[1,1].set_xlabel('Epoch')\n",
    "axes[1,1].set_ylabel('Recall')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df.to_csv('models/model_comparison.csv', index=False)\n",
    "print(\"üíæ Model comparison saved to 'models/model_comparison.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bf4228",
   "metadata": {},
   "source": [
    "## üìà Model Evaluation and Interpretation\n",
    "\n",
    "Detailed analysis of model performance including:\n",
    "- ROC curves and AUC scores\n",
    "- Confusion matrices\n",
    "- Precision-Recall curves\n",
    "- Feature importance (XGBoost)\n",
    "- Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37531fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Model Evaluation\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# ROC Curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# ROC Curve - XGBoost\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
    "axes[0,0].plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {auc_xgb:.3f})', color='blue', linewidth=2)\n",
    "axes[0,0].plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
    "axes[0,0].set_xlabel('False Positive Rate')\n",
    "axes[0,0].set_ylabel('True Positive Rate')\n",
    "axes[0,0].set_title('ROC Curve - XGBoost', fontweight='bold')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# ROC Curve - LSTM\n",
    "fpr_lstm, tpr_lstm, _ = roc_curve(y_test_seq, y_pred_lstm_proba)\n",
    "axes[0,1].plot(fpr_lstm, tpr_lstm, label=f'LSTM (AUC = {auc_lstm:.3f})', color='red', linewidth=2)\n",
    "axes[0,1].plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
    "axes[0,1].set_xlabel('False Positive Rate')\n",
    "axes[0,1].set_ylabel('True Positive Rate')\n",
    "axes[0,1].set_title('ROC Curve - LSTM', fontweight='bold')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve - XGBoost\n",
    "precision_xgb_curve, recall_xgb_curve, _ = precision_recall_curve(y_test, y_pred_proba_xgb)\n",
    "axes[1,0].plot(recall_xgb_curve, precision_xgb_curve, color='blue', linewidth=2)\n",
    "axes[1,0].set_xlabel('Recall')\n",
    "axes[1,0].set_ylabel('Precision')\n",
    "axes[1,0].set_title('Precision-Recall Curve - XGBoost', fontweight='bold')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve - LSTM\n",
    "precision_lstm_curve, recall_lstm_curve, _ = precision_recall_curve(y_test_seq, y_pred_lstm_proba)\n",
    "axes[1,1].plot(recall_lstm_curve, precision_lstm_curve, color='red', linewidth=2)\n",
    "axes[1,1].set_xlabel('Recall')\n",
    "axes[1,1].set_ylabel('Precision')\n",
    "axes[1,1].set_title('Precision-Recall Curve - LSTM', fontweight='bold')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrices Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# XGBoost Confusion Matrix\n",
    "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('XGBoost Confusion Matrix', fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# LSTM Confusion Matrix\n",
    "cm_lstm = confusion_matrix(y_test_seq, y_pred_lstm)\n",
    "sns.heatmap(cm_lstm, annot=True, fmt='d', cmap='Reds', ax=axes[1])\n",
    "axes[1].set_title('LSTM Confusion Matrix', fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification Reports\n",
    "print(\"üìä XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "print(\"\\nüìä LSTM Classification Report:\")\n",
    "print(classification_report(y_test_seq, y_pred_lstm))\n",
    "\n",
    "# Error Analysis\n",
    "print(\"\\nüîç Error Analysis:\")\n",
    "print(f\"XGBoost False Positives: {cm_xgb[0,1]} (predicted flood when no flood)\")\n",
    "print(f\"XGBoost False Negatives: {cm_xgb[1,0]} (missed actual floods)\")\n",
    "print(f\"LSTM False Positives: {cm_lstm[0,1]}\")\n",
    "print(f\"LSTM False Negatives: {cm_lstm[1,0]}\")\n",
    "\n",
    "# Best model selection\n",
    "if f1_xgb > f1_lstm:\n",
    "    best_model = \"XGBoost\"\n",
    "    best_f1 = f1_xgb\n",
    "else:\n",
    "    best_model = \"LSTM\"\n",
    "    best_f1 = f1_lstm\n",
    "\n",
    "print(f\"\\nüèÜ Best performing model: {best_model} (F1-Score: {best_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7587879",
   "metadata": {},
   "source": [
    "## üå¶Ô∏è Live Rainfall Data Fetching via API\n",
    "\n",
    "Integration with weather APIs to get real-time rainfall data:\n",
    "- **OpenWeatherMap**: Historical and current weather data\n",
    "- **MeteoSource**: High-resolution weather data for Bangladesh\n",
    "- **Automatic data processing** for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647268be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live Rainfall Data Fetching Functions\n",
    "\n",
    "def fetch_openweather_historical(lat, lon, days=7, api_key=OPENWEATHER_API_KEY):\n",
    "    \"\"\"\n",
    "    Fetch historical rainfall data from OpenWeatherMap API\n",
    "    \"\"\"\n",
    "    if api_key == \"YOUR_OPENWEATHER_API_KEY\":\n",
    "        print(\"‚ö†Ô∏è Please set your OpenWeatherMap API key!\")\n",
    "        return None\n",
    "    \n",
    "    rainfall_data = []\n",
    "    \n",
    "    for i in range(days):\n",
    "        # Get timestamp for each day\n",
    "        target_date = datetime.now() - timedelta(days=i)\n",
    "        timestamp = int(target_date.timestamp())\n",
    "        \n",
    "        url = f\"{OPENWEATHER_BASE_URL}?lat={lat}&lon={lon}&dt={timestamp}&appid={api_key}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Extract rainfall data\n",
    "                hourly_data = data.get('hourly', [])\n",
    "                daily_rain = 0\n",
    "                \n",
    "                for hour in hourly_data:\n",
    "                    rain_data = hour.get('rain', {})\n",
    "                    daily_rain += rain_data.get('1h', 0)  # 1-hour rainfall in mm\n",
    "                \n",
    "                rainfall_data.append({\n",
    "                    'date': target_date.strftime('%Y-%m-%d'),\n",
    "                    'rainfall': daily_rain\n",
    "                })\n",
    "            else:\n",
    "                print(f\"‚ùå API Error for {target_date.strftime('%Y-%m-%d')}: {data.get('message', 'Unknown error')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Network error: {str(e)}\")\n",
    "    \n",
    "    return pd.DataFrame(rainfall_data).sort_values('date')\n",
    "\n",
    "def fetch_meteosource_data(lat, lon, api_key=METEOSOURCE_API_KEY):\n",
    "    \"\"\"\n",
    "    Fetch current and forecast data from MeteoSource API\n",
    "    \"\"\"\n",
    "    if api_key == \"YOUR_METEOSOURCE_API_KEY\":\n",
    "        print(\"‚ö†Ô∏è Please set your MeteoSource API key!\")\n",
    "        return None\n",
    "    \n",
    "    url = f\"{METEOSOURCE_BASE_URL}?lat={lat}&lon={lon}&sections=current,daily&timezone=UTC&language=en&units=metric&key={api_key}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        data = response.json()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            daily_data = data.get('daily', {}).get('data', [])\n",
    "            \n",
    "            rainfall_data = []\n",
    "            for day in daily_data[:7]:  # Last 7 days\n",
    "                rainfall_data.append({\n",
    "                    'date': day['day'],\n",
    "                    'rainfall': day.get('precipitation', {}).get('total', 0)\n",
    "                })\n",
    "            \n",
    "            return pd.DataFrame(rainfall_data)\n",
    "        else:\n",
    "            print(f\"‚ùå MeteoSource API Error: {data.get('message', 'Unknown error')}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Network error: {str(e)}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_simulated_live_data(location='Dhaka', days=7):\n",
    "    \"\"\"\n",
    "    Simulate live rainfall data for demonstration\n",
    "    (Use when API keys are not available)\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Simulating live rainfall data for {location}...\")\n",
    "    \n",
    "    dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days)]\n",
    "    dates.reverse()\n",
    "    \n",
    "    # Simulate realistic rainfall patterns\n",
    "    np.random.seed(int(datetime.now().timestamp()) % 1000)\n",
    "    rainfall = np.random.gamma(2, 3, days)  # Gamma distribution\n",
    "    rainfall = np.maximum(rainfall, 0)  # No negative values\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'rainfall': rainfall\n",
    "    })\n",
    "\n",
    "# Fetch Live Rainfall Data\n",
    "location = 'Dhaka'\n",
    "lat, lon = LOCATIONS[location]\n",
    "\n",
    "print(f\"üå¶Ô∏è Fetching live rainfall data for {location} ({lat}, {lon})\")\n",
    "\n",
    "# Try OpenWeatherMap first, then MeteoSource, then simulation\n",
    "live_rainfall = fetch_openweather_historical(lat, lon, days=7)\n",
    "\n",
    "if live_rainfall is None:\n",
    "    live_rainfall = fetch_meteosource_data(lat, lon)\n",
    "\n",
    "if live_rainfall is None:\n",
    "    live_rainfall = get_simulated_live_data(location, days=7)\n",
    "\n",
    "print(f\"‚úÖ Retrieved {len(live_rainfall)} days of rainfall data\")\n",
    "print(\"\\nüìä Live Rainfall Data:\")\n",
    "print(live_rainfall)\n",
    "\n",
    "# Add water level estimates (simplified simulation)\n",
    "# In production, this would come from river monitoring stations\n",
    "base_level = 4.2\n",
    "live_rainfall['estimated_water_level'] = base_level + (live_rainfall['rainfall'] * 0.1) + np.random.normal(0, 0.2, len(live_rainfall))\n",
    "live_rainfall['estimated_water_level'] = np.maximum(live_rainfall['estimated_water_level'], 2.0)\n",
    "\n",
    "print(\"\\nüåä With estimated water levels:\")\n",
    "print(live_rainfall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9153bd",
   "metadata": {},
   "source": [
    "## üéØ Predicting Floods with Latest Data\n",
    "\n",
    "Using the trained models to predict flood risk based on the latest rainfall data:\n",
    "1. **Feature Engineering**: Create lag features from live data\n",
    "2. **Model Prediction**: Use both XGBoost and LSTM models\n",
    "3. **Risk Assessment**: Combine predictions for final assessment\n",
    "4. **Confidence Scoring**: Provide confidence levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e65d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live Flood Prediction Function\n",
    "def predict_flood_risk(rainfall_data, models_dict, scaler, feature_cols, flood_threshold=5.5):\n",
    "    \"\"\"\n",
    "    Predict flood risk using live rainfall data\n",
    "    \n",
    "    Parameters:\n",
    "    - rainfall_data: DataFrame with 'date', 'rainfall', 'estimated_water_level'\n",
    "    - models_dict: Dictionary containing trained models\n",
    "    - scaler: Fitted StandardScaler\n",
    "    - feature_cols: List of feature column names\n",
    "    - flood_threshold: Water level threshold for flood\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare data similar to training\n",
    "    df = rainfall_data.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Add temporal features\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['season'] = df['month'].apply(lambda x: 'monsoon' if 6 <= x <= 9 else 'dry')\n",
    "    \n",
    "    # Create same features as training\n",
    "    try:\n",
    "        df_features = create_flood_features(df.rename(columns={'estimated_water_level': 'water_level'}), \n",
    "                                          flood_threshold, lag_days=7)\n",
    "        \n",
    "        if len(df_features) == 0:\n",
    "            return None, \"Not enough data for prediction\"\n",
    "        \n",
    "        # Get the latest row for prediction\n",
    "        latest_features = df_features[feature_cols].iloc[-1:].values\n",
    "        latest_features_scaled = scaler.transform(latest_features)\n",
    "        \n",
    "        predictions = {}\n",
    "        \n",
    "        # XGBoost prediction\n",
    "        if 'xgboost' in models_dict:\n",
    "            xgb_prob = models_dict['xgboost'].predict_proba(latest_features)[0, 1]\n",
    "            xgb_pred = models_dict['xgboost'].predict(latest_features)[0]\n",
    "            predictions['xgboost'] = {\n",
    "                'probability': xgb_prob,\n",
    "                'prediction': xgb_pred,\n",
    "                'confidence': max(xgb_prob, 1-xgb_prob)  # Distance from 0.5\n",
    "            }\n",
    "        \n",
    "        # LSTM prediction (if we have enough sequence data)\n",
    "        if 'lstm' in models_dict and len(df_features) >= 7:\n",
    "            # Create sequence for LSTM\n",
    "            sequence_data = df_features[feature_cols].iloc[-7:].values\n",
    "            sequence_scaled = scaler.transform(sequence_data)\n",
    "            sequence_input = sequence_scaled.reshape(1, 7, len(feature_cols))\n",
    "            \n",
    "            lstm_prob = models_dict['lstm'].predict(sequence_input)[0, 0]\n",
    "            lstm_pred = int(lstm_prob > 0.5)\n",
    "            predictions['lstm'] = {\n",
    "                'probability': lstm_prob,\n",
    "                'prediction': lstm_pred,\n",
    "                'confidence': max(lstm_prob, 1-lstm_prob)\n",
    "            }\n",
    "        \n",
    "        # Ensemble prediction (average of available models)\n",
    "        if predictions:\n",
    "            avg_prob = np.mean([p['probability'] for p in predictions.values()])\n",
    "            ensemble_pred = int(avg_prob > 0.5)\n",
    "            ensemble_confidence = max(avg_prob, 1-avg_prob)\n",
    "            \n",
    "            predictions['ensemble'] = {\n",
    "                'probability': avg_prob,\n",
    "                'prediction': ensemble_pred,\n",
    "                'confidence': ensemble_confidence\n",
    "            }\n",
    "        \n",
    "        return predictions, df_features.iloc[-1]\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, f\"Error in prediction: {str(e)}\"\n",
    "\n",
    "# Load trained models\n",
    "try:\n",
    "    models = {\n",
    "        'xgboost': joblib.load('models/xgboost_flood_model.pkl'),\n",
    "        'lstm': tf.keras.models.load_model('models/lstm_flood_model.h5')\n",
    "    }\n",
    "    loaded_scaler = joblib.load('models/feature_scaler.pkl')\n",
    "    print(\"‚úÖ Models loaded successfully!\")\n",
    "except:\n",
    "    # Use the models we just trained\n",
    "    models = {\n",
    "        'xgboost': xgb_model,\n",
    "        'lstm': lstm_model\n",
    "    }\n",
    "    loaded_scaler = scaler\n",
    "    print(\"‚úÖ Using freshly trained models!\")\n",
    "\n",
    "# Make Prediction\n",
    "print(f\"\\nüîÆ Making flood prediction for {location}...\")\n",
    "predictions, latest_data = predict_flood_risk(\n",
    "    live_rainfall, models, loaded_scaler, feature_cols, \n",
    "    flood_threshold=FLOOD_THRESHOLDS.get(location, 5.5)\n",
    ")\n",
    "\n",
    "if predictions:\n",
    "    print(f\"\\nüéØ Flood Prediction Results for {location}:\")\n",
    "    print(f\"üìÖ Latest data date: {live_rainfall['date'].iloc[-1]}\")\n",
    "    print(f\"üåßÔ∏è Recent rainfall: {live_rainfall['rainfall'].iloc[-1]:.1f} mm\")\n",
    "    print(f\"üåä Estimated water level: {live_rainfall['estimated_water_level'].iloc[-1]:.2f} m\")\n",
    "    print(f\"üö® Flood threshold: {FLOOD_THRESHOLDS.get(location, 5.5)} m\")\n",
    "    \n",
    "    print(f\"\\nüìä Model Predictions:\")\n",
    "    for model_name, pred in predictions.items():\n",
    "        risk_level = \"üî¥ HIGH RISK\" if pred['prediction'] == 1 else \"üü¢ LOW RISK\"\n",
    "        print(f\"   ‚Ä¢ {model_name.upper()}: {risk_level}\")\n",
    "        print(f\"     - Probability: {pred['probability']:.3f}\")\n",
    "        print(f\"     - Confidence: {pred['confidence']:.3f}\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    ensemble_pred = predictions.get('ensemble', predictions[list(predictions.keys())[0]])\n",
    "    \n",
    "    if ensemble_pred['prediction'] == 1:\n",
    "        print(f\"\\nüö® FLOOD WARNING!\")\n",
    "        print(f\"   Risk Level: HIGH ({ensemble_pred['probability']:.1%})\")\n",
    "        print(f\"   Confidence: {ensemble_pred['confidence']:.1%}\")\n",
    "        alert_needed = True\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ No immediate flood risk\")\n",
    "        print(f\"   Risk Level: LOW ({ensemble_pred['probability']:.1%})\")\n",
    "        print(f\"   Confidence: {ensemble_pred['confidence']:.1%}\")\n",
    "        alert_needed = False\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Could not make prediction: {latest_data}\")\n",
    "    alert_needed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de655fcd",
   "metadata": {},
   "source": [
    "## üìù CSV Logger for Daily Predictions\n",
    "\n",
    "Automatic logging system to track predictions over time:\n",
    "- **Prediction History**: Store all predictions with timestamps\n",
    "- **Performance Tracking**: Monitor accuracy over time\n",
    "- **Data Backup**: Keep records for analysis and model improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b1168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Logger for Predictions\n",
    "def log_prediction(location, rainfall_data, predictions, log_file='logs/flood_predictions.csv'):\n",
    "    \"\"\"\n",
    "    Log prediction results to CSV file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create log entry\n",
    "    log_entry = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'location': location,\n",
    "        'date': rainfall_data['date'].iloc[-1],\n",
    "        'recent_rainfall': rainfall_data['rainfall'].iloc[-1],\n",
    "        'estimated_water_level': rainfall_data['estimated_water_level'].iloc[-1],\n",
    "        'flood_threshold': FLOOD_THRESHOLDS.get(location, 5.5)\n",
    "    }\n",
    "    \n",
    "    # Add model predictions\n",
    "    if predictions:\n",
    "        for model_name, pred in predictions.items():\n",
    "            log_entry[f'{model_name}_probability'] = pred['probability']\n",
    "            log_entry[f'{model_name}_prediction'] = pred['prediction']\n",
    "            log_entry[f'{model_name}_confidence'] = pred['confidence']\n",
    "    \n",
    "    # Create DataFrame\n",
    "    log_df = pd.DataFrame([log_entry])\n",
    "    \n",
    "    # Check if log file exists\n",
    "    if os.path.exists(log_file):\n",
    "        # Append to existing file\n",
    "        log_df.to_csv(log_file, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Create new file with headers\n",
    "        log_df.to_csv(log_file, mode='w', header=True, index=False)\n",
    "    \n",
    "    print(f\"üìù Prediction logged to {log_file}\")\n",
    "\n",
    "def view_prediction_history(log_file='logs/flood_predictions.csv', days=30):\n",
    "    \"\"\"\n",
    "    View recent prediction history\n",
    "    \"\"\"\n",
    "    if not os.path.exists(log_file):\n",
    "        print(\"üì≠ No prediction history found\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(log_file)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Filter recent predictions\n",
    "    cutoff_date = datetime.now() - timedelta(days=days)\n",
    "    recent_df = df[df['timestamp'] >= cutoff_date].copy()\n",
    "    \n",
    "    if len(recent_df) == 0:\n",
    "        print(f\"üì≠ No predictions found in the last {days} days\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìä Prediction History (Last {days} days):\")\n",
    "    print(f\"   ‚Ä¢ Total predictions: {len(recent_df)}\")\n",
    "    \n",
    "    if 'ensemble_prediction' in recent_df.columns:\n",
    "        flood_predictions = recent_df['ensemble_prediction'].sum()\n",
    "        print(f\"   ‚Ä¢ Flood warnings: {flood_predictions}\")\n",
    "        print(f\"   ‚Ä¢ Warning rate: {flood_predictions/len(recent_df)*100:.1f}%\")\n",
    "    \n",
    "    return recent_df\n",
    "\n",
    "# Log Current Prediction\n",
    "if predictions:\n",
    "    log_prediction(location, live_rainfall, predictions)\n",
    "    \n",
    "    # View recent history\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    history = view_prediction_history(days=30)\n",
    "    \n",
    "    if history is not None and len(history) > 1:\n",
    "        print(\"\\nüìà Recent Prediction Trends:\")\n",
    "        \n",
    "        # Plot prediction history\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Rainfall and water level trends\n",
    "        axes[0].plot(pd.to_datetime(history['date']), history['recent_rainfall'], \n",
    "                    marker='o', label='Rainfall (mm)', color='blue', alpha=0.7)\n",
    "        ax0_twin = axes[0].twinx()\n",
    "        ax0_twin.plot(pd.to_datetime(history['date']), history['estimated_water_level'], \n",
    "                     marker='s', label='Water Level (m)', color='green', alpha=0.7)\n",
    "        ax0_twin.axhline(y=history['flood_threshold'].iloc[0], color='red', \n",
    "                        linestyle='--', label='Flood Threshold')\n",
    "        \n",
    "        axes[0].set_xlabel('Date')\n",
    "        axes[0].set_ylabel('Rainfall (mm)', color='blue')\n",
    "        ax0_twin.set_ylabel('Water Level (m)', color='green')\n",
    "        axes[0].set_title('Rainfall and Water Level Trends', fontweight='bold')\n",
    "        axes[0].legend(loc='upper left')\n",
    "        ax0_twin.legend(loc='upper right')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Prediction probabilities\n",
    "        if 'ensemble_probability' in history.columns:\n",
    "            axes[1].plot(pd.to_datetime(history['date']), history['ensemble_probability'], \n",
    "                        marker='o', color='red', linewidth=2, label='Flood Probability')\n",
    "            axes[1].axhline(y=0.5, color='black', linestyle='--', alpha=0.5, label='Decision Threshold')\n",
    "            axes[1].fill_between(pd.to_datetime(history['date']), history['ensemble_probability'], \n",
    "                               0.5, where=(history['ensemble_probability'] > 0.5), \n",
    "                               color='red', alpha=0.3, label='High Risk')\n",
    "        \n",
    "        axes[1].set_xlabel('Date')\n",
    "        axes[1].set_ylabel('Flood Probability')\n",
    "        axes[1].set_title('Flood Risk Predictions Over Time', fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].set_ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Set up automated logging (for production)\n",
    "def setup_automated_logging():\n",
    "    \"\"\"\n",
    "    Set up automated logging that runs daily\n",
    "    This would typically be called by a cron job or scheduler\n",
    "    \"\"\"\n",
    "    \n",
    "    script_content = f'''#!/usr/bin/env python3\n",
    "import sys\n",
    "sys.path.append('{os.getcwd()}')\n",
    "\n",
    "# Import required modules and run prediction\n",
    "from flood_prediction_system import *\n",
    "\n",
    "# Run daily prediction and logging\n",
    "location = \"Dhaka\"\n",
    "lat, lon = LOCATIONS[location]\n",
    "live_data = get_simulated_live_data(location, days=7)\n",
    "predictions, _ = predict_flood_risk(live_data, models, scaler, feature_cols)\n",
    "\n",
    "if predictions:\n",
    "    log_prediction(location, live_data, predictions)\n",
    "    print(f\"Daily prediction logged for {{location}}\")\n",
    "'''\n",
    "    \n",
    "    with open('logs/daily_prediction.py', 'w') as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    print(\"üìÖ Automated logging script created: 'logs/daily_prediction.py'\")\n",
    "    print(\"üí° To run daily, add to crontab: 0 6 * * * python3 /path/to/logs/daily_prediction.py\")\n",
    "\n",
    "setup_automated_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3a092",
   "metadata": {},
   "source": [
    "## üö® Telegram/Email Alert Integration\n",
    "\n",
    "Automated alert system that sends notifications when flood risk is detected:\n",
    "- **Telegram Bot**: Instant messaging alerts\n",
    "- **Email Notifications**: Detailed reports via email\n",
    "- **Smart Alerting**: Prevents spam with cooldown periods\n",
    "- **Rich Content**: Maps, charts, and detailed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc138c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alert System Implementation\n",
    "\n",
    "def send_telegram_alert(message, bot_token=TELEGRAM_BOT_TOKEN, chat_id=TELEGRAM_CHAT_ID):\n",
    "    \"\"\"\n",
    "    Send alert via Telegram Bot\n",
    "    \"\"\"\n",
    "    if bot_token == \"YOUR_TELEGRAM_BOT_TOKEN\" or chat_id == \"YOUR_TELEGRAM_CHAT_ID\":\n",
    "        print(\"‚ö†Ô∏è Telegram credentials not configured. Skipping Telegram alert.\")\n",
    "        return False\n",
    "    \n",
    "    url = f\"https://api.telegram.org/bot{bot_token}/sendMessage\"\n",
    "    \n",
    "    payload = {\n",
    "        'chat_id': chat_id,\n",
    "        'text': message,\n",
    "        'parse_mode': 'Markdown'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Telegram alert sent successfully!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Telegram alert failed: {response.text}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Telegram error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def send_email_alert(subject, body, config=EMAIL_CONFIG):\n",
    "    \"\"\"\n",
    "    Send alert via Email\n",
    "    \"\"\"\n",
    "    if config['sender_email'] == 'YOUR_EMAIL@gmail.com':\n",
    "        print(\"‚ö†Ô∏è Email credentials not configured. Skipping email alert.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Create message\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = config['sender_email']\n",
    "        msg['To'] = config['recipient_email']\n",
    "        msg['Subject'] = subject\n",
    "        \n",
    "        msg.attach(MIMEText(body, 'plain'))\n",
    "        \n",
    "        # Send email\n",
    "        server = smtplib.SMTP(config['smtp_server'], config['port'])\n",
    "        server.starttls()\n",
    "        server.login(config['sender_email'], config['sender_password'])\n",
    "        \n",
    "        text = msg.as_string()\n",
    "        server.sendmail(config['sender_email'], config['recipient_email'], text)\n",
    "        server.quit()\n",
    "        \n",
    "        print(\"‚úÖ Email alert sent successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Email error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def create_flood_alert_message(location, predictions, rainfall_data, threshold):\n",
    "    \"\"\"\n",
    "    Create formatted alert message\n",
    "    \"\"\"\n",
    "    ensemble_pred = predictions.get('ensemble', predictions[list(predictions.keys())[0]])\n",
    "    \n",
    "    # Emoji based on risk level\n",
    "    risk_emoji = \"üö®üî¥\" if ensemble_pred['prediction'] == 1 else \"‚úÖüü¢\"\n",
    "    \n",
    "    message = f\"\"\"\n",
    "{risk_emoji} **FLOOD ALERT - {location.upper()}** {risk_emoji}\n",
    "\n",
    "üìÖ **Date**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n",
    "üìç **Location**: {location}\n",
    "üåßÔ∏è **Recent Rainfall**: {rainfall_data['rainfall'].iloc[-1]:.1f} mm\n",
    "üåä **Water Level**: {rainfall_data['estimated_water_level'].iloc[-1]:.2f} m\n",
    "üö® **Flood Threshold**: {threshold} m\n",
    "\n",
    "üìä **AI Predictions**:\n",
    "\"\"\"\n",
    "    \n",
    "    for model_name, pred in predictions.items():\n",
    "        risk_text = \"HIGH RISK\" if pred['prediction'] == 1 else \"LOW RISK\"\n",
    "        message += f\"‚Ä¢ {model_name.upper()}: {risk_text} ({pred['probability']:.1%})\\n\"\n",
    "    \n",
    "    if ensemble_pred['prediction'] == 1:\n",
    "        message += f\"\"\"\n",
    "‚ö†Ô∏è **FLOOD WARNING ISSUED**\n",
    "Risk Level: HIGH ({ensemble_pred['probability']:.1%})\n",
    "Confidence: {ensemble_pred['confidence']:.1%}\n",
    "\n",
    "üèÉ‚Äç‚ôÇÔ∏è **Recommended Actions**:\n",
    "‚Ä¢ Monitor water levels closely\n",
    "‚Ä¢ Prepare evacuation if necessary\n",
    "‚Ä¢ Move valuables to higher ground\n",
    "‚Ä¢ Stay informed through official channels\n",
    "\"\"\"\n",
    "    else:\n",
    "        message += f\"\"\"\n",
    "‚úÖ **No Immediate Flood Risk**\n",
    "Risk Level: LOW ({ensemble_pred['probability']:.1%})\n",
    "Confidence: {ensemble_pred['confidence']:.1%}\n",
    "\n",
    "üìã **Current Status**: Normal\n",
    "Continue monitoring conditions.\n",
    "\"\"\"\n",
    "    \n",
    "    message += f\"\\\\nü§ñ *Generated by AI Flood Prediction System*\"\n",
    "    \n",
    "    return message\n",
    "\n",
    "def check_alert_cooldown(location, cooldown_hours=6):\n",
    "    \"\"\"\n",
    "    Check if enough time has passed since last alert to prevent spam\n",
    "    \"\"\"\n",
    "    cooldown_file = f'alerts/last_alert_{location.lower()}.txt'\n",
    "    \n",
    "    if os.path.exists(cooldown_file):\n",
    "        try:\n",
    "            with open(cooldown_file, 'r') as f:\n",
    "                last_alert_time = datetime.fromisoformat(f.read().strip())\n",
    "            \n",
    "            time_diff = datetime.now() - last_alert_time\n",
    "            if time_diff.total_seconds() < cooldown_hours * 3600:\n",
    "                remaining = cooldown_hours * 3600 - time_diff.total_seconds()\n",
    "                print(f\"‚è∞ Alert cooldown active. {remaining/3600:.1f} hours remaining.\")\n",
    "                return False\n",
    "        except:\n",
    "            pass  # File corrupted or doesn't exist\n",
    "    \n",
    "    # Update last alert time\n",
    "    with open(cooldown_file, 'w') as f:\n",
    "        f.write(datetime.now().isoformat())\n",
    "    \n",
    "    return True\n",
    "\n",
    "def send_flood_alert(location, predictions, rainfall_data, threshold, force=False):\n",
    "    \"\"\"\n",
    "    Send flood alert via all configured channels\n",
    "    \"\"\"\n",
    "    if not force and not check_alert_cooldown(location, cooldown_hours=6):\n",
    "        return False\n",
    "    \n",
    "    # Create alert message\n",
    "    message = create_flood_alert_message(location, predictions, rainfall_data, threshold)\n",
    "    \n",
    "    print(f\"üö® Sending flood alert for {location}...\")\n",
    "    \n",
    "    # Send alerts\n",
    "    telegram_sent = send_telegram_alert(message)\n",
    "    \n",
    "    # Email with more detailed info\n",
    "    email_subject = f\"üö® Flood Alert: {location} - {datetime.now().strftime('%Y-%m-%d')}\"\n",
    "    email_sent = send_email_alert(email_subject, message.replace('*', '').replace('`', ''))\n",
    "    \n",
    "    if telegram_sent or email_sent:\n",
    "        print(\"‚úÖ Alert sent successfully!\")\n",
    "        \n",
    "        # Log alert\n",
    "        alert_log = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'location': location,\n",
    "            'alert_type': 'flood_warning' if predictions.get('ensemble', {}).get('prediction', 0) == 1 else 'status_update',\n",
    "            'telegram_sent': telegram_sent,\n",
    "            'email_sent': email_sent,\n",
    "            'risk_probability': predictions.get('ensemble', {}).get('probability', 0)\n",
    "        }\n",
    "        \n",
    "        alert_df = pd.DataFrame([alert_log])\n",
    "        alert_file = 'alerts/alert_history.csv'\n",
    "        \n",
    "        if os.path.exists(alert_file):\n",
    "            alert_df.to_csv(alert_file, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            alert_df.to_csv(alert_file, mode='w', header=True, index=False)\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå Failed to send alerts\")\n",
    "        return False\n",
    "\n",
    "# Send Alert if High Risk Detected\n",
    "if predictions and alert_needed:\n",
    "    threshold = FLOOD_THRESHOLDS.get(location, 5.5)\n",
    "    alert_success = send_flood_alert(location, predictions, live_rainfall, threshold)\n",
    "    \n",
    "    if alert_success:\n",
    "        print(\"üö® Emergency alert sent to all configured channels!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Alert system encountered issues\")\n",
    "        \n",
    "elif predictions:\n",
    "    print(\"üì± No alert needed - flood risk is low\")\n",
    "    \n",
    "    # Optional: Send daily status update (uncomment to enable)\n",
    "    # send_flood_alert(location, predictions, live_rainfall, threshold, force=True)\n",
    "\n",
    "# View Alert History\n",
    "def view_alert_history(days=30):\n",
    "    \"\"\"View recent alert history\"\"\"\n",
    "    alert_file = 'alerts/alert_history.csv'\n",
    "    \n",
    "    if not os.path.exists(alert_file):\n",
    "        print(\"üì≠ No alert history found\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_csv(alert_file)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Filter recent alerts\n",
    "    cutoff_date = datetime.now() - timedelta(days=days)\n",
    "    recent_df = df[df['timestamp'] >= cutoff_date]\n",
    "    \n",
    "    if len(recent_df) == 0:\n",
    "        print(f\"üì≠ No alerts sent in the last {days} days\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìä Alert History (Last {days} days):\")\n",
    "    print(f\"   ‚Ä¢ Total alerts: {len(recent_df)}\")\n",
    "    print(f\"   ‚Ä¢ Flood warnings: {(recent_df['alert_type'] == 'flood_warning').sum()}\")\n",
    "    print(f\"   ‚Ä¢ Status updates: {(recent_df['alert_type'] == 'status_update').sum()}\")\n",
    "    print(f\"   ‚Ä¢ Telegram success rate: {recent_df['telegram_sent'].mean()*100:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Email success rate: {recent_df['email_sent'].mean()*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\\\nüìã Recent Alerts:\")\n",
    "    for _, row in recent_df.tail(5).iterrows():\n",
    "        alert_type = \"üö®\" if row['alert_type'] == 'flood_warning' else \"üìä\"\n",
    "        print(f\"   {alert_type} {row['timestamp'].strftime('%Y-%m-%d %H:%M')} - {row['location']} ({row['risk_probability']:.1%})\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "view_alert_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ef160",
   "metadata": {},
   "source": [
    "## üéâ System Summary and Next Steps\n",
    "\n",
    "### ‚úÖ What We've Built:\n",
    "\n",
    "1. **ü§ñ AI Models**: XGBoost and LSTM models for flood prediction\n",
    "2. **üì° Live Data**: API integration for real-time rainfall data\n",
    "3. **üö® Alert System**: Telegram and email notifications\n",
    "4. **üìä Logging**: CSV tracking of predictions and alerts\n",
    "5. **üìà Monitoring**: Historical analysis and trend visualization\n",
    "\n",
    "### üöÄ Deployment Ready Features:\n",
    "\n",
    "- **Production Models**: Trained and saved models ready for deployment\n",
    "- **API Integration**: OpenWeatherMap and MeteoSource support\n",
    "- **Automated Logging**: Daily prediction tracking\n",
    "- **Smart Alerts**: Cooldown system to prevent spam\n",
    "- **Error Handling**: Robust error handling and fallbacks\n",
    "\n",
    "### üìù To Get Started:\n",
    "\n",
    "1. **Get API Keys**:\n",
    "   - OpenWeatherMap: https://openweathermap.org/api\n",
    "   - MeteoSource: https://www.meteosource.com/\n",
    "\n",
    "2. **Set up Alerts**:\n",
    "   - Create Telegram Bot: @BotFather on Telegram\n",
    "   - Configure email settings (Gmail App Password recommended)\n",
    "\n",
    "3. **Deploy**:\n",
    "   - Set up cron job for daily predictions\n",
    "   - Monitor logs and adjust thresholds as needed\n",
    "\n",
    "### üîß Customization Options:\n",
    "\n",
    "- **Add More Locations**: Update `LOCATIONS` and `FLOOD_THRESHOLDS`\n",
    "- **Adjust Models**: Retrain with local data for better accuracy\n",
    "- **Custom Alerts**: Modify alert messages and delivery methods\n",
    "- **Dashboard**: Create web interface for real-time monitoring\n",
    "\n",
    "---\n",
    "\n",
    "**üåä Your Bangladesh Flood Prediction System is ready to save lives!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
