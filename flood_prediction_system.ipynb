{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d417066",
   "metadata": {},
   "source": [
    "# 🌊 Bangladesh Flood Prediction System\n",
    "\n",
    "## 📋 Project Overview\n",
    "\n",
    "This comprehensive system predicts river flooding in Bangladesh based on:\n",
    "- **Rainfall data** from the past N days\n",
    "- **River water levels** from monitoring stations\n",
    "- **Live API integration** for real-time predictions\n",
    "- **Alert system** via Telegram/Email\n",
    "- **CSV logging** for historical tracking\n",
    "\n",
    "### 🎯 Objectives\n",
    "1. Build XGBoost and LSTM models for flood prediction\n",
    "2. Integrate live rainfall data from weather APIs\n",
    "3. Create automated alert system\n",
    "4. Log predictions for monitoring and analysis\n",
    "\n",
    "### 📊 Data Sources\n",
    "- **Rainfall**: OpenWeatherMap API, MeteoSource API\n",
    "- **River Levels**: BWDB (Bangladesh Water Development Board)\n",
    "- **Historical Data**: Kaggle Bangladesh flood datasets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448c5de0",
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/digantohaque/python/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <98D50080-9632-3EA4-B874-146E55453763> /Users/digantohaque/python/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Machine Learning Libraries\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgb\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python/.venv/lib/python3.13/site-packages/xgboost/__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     Booster,\n\u001b[32m     10\u001b[39m     DataIter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     build_info,\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python/.venv/lib/python3.13/site-packages/xgboost/tracker.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, _deprecate_positional_args, make_jcargs\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python/.venv/lib/python3.13/site-packages/xgboost/core.py:295\u001b[39m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m _LIB = \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    299\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    300\u001b[39m \n\u001b[32m    301\u001b[39m \u001b[33;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m \u001b[33;03m        return value from API calls\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python/.venv/lib/python3.13/site-packages/xgboost/core.py:257\u001b[39m, in \u001b[36m_load_lib\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[32m    256\u001b[39m         libname = os.path.basename(lib_paths[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[32m    258\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[33mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) could not be loaded.\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[33mLikely causes:\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[33m  * OpenMP runtime is not installed\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[33m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[33m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[33m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[33m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[32m    266\u001b[39m \n\u001b[32m    267\u001b[39m \u001b[33m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[32m    268\u001b[39m \n\u001b[32m    269\u001b[39m \u001b[33mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    271\u001b[39m         )\n\u001b[32m    272\u001b[39m     _register_log_callback(lib)\n\u001b[32m    274\u001b[39m     libver = _lib_version(lib)\n",
      "\u001b[31mXGBoostError\u001b[39m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/digantohaque/python/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <98D50080-9632-3EA4-B874-146E55453763> /Users/digantohaque/python/.venv/lib/python3.13/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries and Set Up Environment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# API and Web Libraries\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Alert and Logging Libraries\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"📦 XGBoost version: {xgb.__version__}\")\n",
    "print(f\"🧠 TensorFlow version: {tf.__version__}\")\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Create directories for logs and data\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('alerts', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55acbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and API Keys\n",
    "# ⚠️ IMPORTANT: Replace with your actual API keys\n",
    "\n",
    "# OpenWeatherMap API Configuration\n",
    "OPENWEATHER_API_KEY = \"YOUR_OPENWEATHER_API_KEY\"\n",
    "OPENWEATHER_BASE_URL = \"https://api.openweathermap.org/data/2.5/onecall/timemachine\"\n",
    "\n",
    "# MeteoSource API Configuration (Alternative)\n",
    "METEOSOURCE_API_KEY = \"YOUR_METEOSOURCE_API_KEY\"\n",
    "METEOSOURCE_BASE_URL = \"https://www.meteosource.com/api/v1/free/point\"\n",
    "\n",
    "# Bangladesh key locations (lat, lon)\n",
    "LOCATIONS = {\n",
    "    'Dhaka': (23.8103, 90.4125),\n",
    "    'Sylhet': (24.8949, 91.8687),\n",
    "    'Rangpur': (25.7439, 89.2752),\n",
    "    'Bahadurabad': (25.1906, 89.7006),  # Major river gauge station\n",
    "    'Chittagong': (22.3569, 91.7832)\n",
    "}\n",
    "\n",
    "# Flood thresholds (in meters) - Based on BWDB danger levels\n",
    "FLOOD_THRESHOLDS = {\n",
    "    'Dhaka': 5.5,\n",
    "    'Sylhet': 6.0,\n",
    "    'Rangpur': 4.8,\n",
    "    'Bahadurabad': 7.2,\n",
    "    'Chittagong': 3.5\n",
    "}\n",
    "\n",
    "# Alert Configuration\n",
    "TELEGRAM_BOT_TOKEN = \"YOUR_TELEGRAM_BOT_TOKEN\"\n",
    "TELEGRAM_CHAT_ID = \"YOUR_TELEGRAM_CHAT_ID\"\n",
    "\n",
    "EMAIL_CONFIG = {\n",
    "    'smtp_server': 'smtp.gmail.com',\n",
    "    'port': 587,\n",
    "    'sender_email': 'YOUR_EMAIL@gmail.com',\n",
    "    'sender_password': 'YOUR_APP_PASSWORD',\n",
    "    'recipient_email': 'ALERT_RECIPIENT@gmail.com'\n",
    "}\n",
    "\n",
    "print(\"⚙️ Configuration loaded successfully!\")\n",
    "print(f\"📍 Monitoring {len(LOCATIONS)} locations in Bangladesh\")\n",
    "print(f\"🚨 Flood thresholds configured for {len(FLOOD_THRESHOLDS)} stations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393f15f8",
   "metadata": {},
   "source": [
    "## 📥 Data Acquisition and Loading\n",
    "\n",
    "### Dataset Sources:\n",
    "1. **Historical Rainfall Data**: Bangladesh meteorological data\n",
    "2. **River Water Levels**: BWDB monitoring stations\n",
    "3. **Flood Records**: Historical flood events (2019-2024)\n",
    "\n",
    "You can download datasets from:\n",
    "- **Kaggle**: \"Regression-Based Flood Prediction in Bangladesh\"\n",
    "- **BWDB**: Bangladesh Water Development Board\n",
    "- **BMD**: Bangladesh Meteorological Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Synthetic Historical Data for Demonstration\n",
    "# In production, replace this with actual data loading\n",
    "\n",
    "def generate_synthetic_data(days=1095):  # 3 years of data\n",
    "    \"\"\"Generate synthetic rainfall and river level data for Bangladesh\"\"\"\n",
    "    \n",
    "    # Date range\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=days)\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Simulate seasonal patterns (monsoon season: June-September)\n",
    "    seasonal_factor = np.array([\n",
    "        1.5 if 6 <= date.month <= 9 else 0.8 \n",
    "        for date in dates\n",
    "    ])\n",
    "    \n",
    "    # Generate rainfall data (mm/day)\n",
    "    base_rainfall = np.random.gamma(2, 2, len(dates))  # Gamma distribution for rainfall\n",
    "    rainfall = base_rainfall * seasonal_factor + np.random.normal(0, 2, len(dates))\n",
    "    rainfall = np.maximum(rainfall, 0)  # No negative rainfall\n",
    "    \n",
    "    # Generate river water level (meters)\n",
    "    # Water level correlates with rainfall with some lag\n",
    "    base_level = 3.5  # Base water level\n",
    "    level_response = np.convolve(rainfall, [0.1, 0.2, 0.3, 0.2, 0.1, 0.05, 0.05], mode='same')\n",
    "    water_level = base_level + level_response + np.random.normal(0, 0.3, len(dates))\n",
    "    water_level = np.maximum(water_level, 1.0)  # Minimum water level\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'rainfall': rainfall,\n",
    "        'water_level': water_level,\n",
    "        'month': [d.month for d in dates],\n",
    "        'year': [d.year for d in dates],\n",
    "        'season': ['monsoon' if 6 <= d.month <= 9 else 'dry' for d in dates]\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate synthetic data\n",
    "print(\"🔄 Generating synthetic historical data...\")\n",
    "historical_data = generate_synthetic_data(days=1095)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"📊 Dataset shape: {historical_data.shape}\")\n",
    "print(f\"📅 Date range: {historical_data['date'].min()} to {historical_data['date'].max()}\")\n",
    "print(\"\\n📋 First 5 rows:\")\n",
    "print(historical_data.head())\n",
    "\n",
    "print(\"\\n📈 Statistical Summary:\")\n",
    "print(historical_data[['rainfall', 'water_level']].describe())\n",
    "\n",
    "# Save to CSV for future use\n",
    "historical_data.to_csv('data/historical_flood_data.csv', index=False)\n",
    "print(\"💾 Data saved to 'data/historical_flood_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb01fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Rainfall time series\n",
    "axes[0,0].plot(historical_data['date'], historical_data['rainfall'], alpha=0.7, color='blue')\n",
    "axes[0,0].set_title('Daily Rainfall (mm)', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Rainfall (mm)')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Water level time series\n",
    "axes[0,1].plot(historical_data['date'], historical_data['water_level'], alpha=0.7, color='green')\n",
    "axes[0,1].axhline(y=5.5, color='red', linestyle='--', label='Flood Threshold (5.5m)')\n",
    "axes[0,1].set_title('River Water Level (m)', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_ylabel('Water Level (m)')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Monthly rainfall distribution\n",
    "monthly_rain = historical_data.groupby('month')['rainfall'].mean()\n",
    "axes[1,0].bar(monthly_rain.index, monthly_rain.values, color='skyblue', alpha=0.8)\n",
    "axes[1,0].set_title('Average Monthly Rainfall', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Month')\n",
    "axes[1,0].set_ylabel('Average Rainfall (mm)')\n",
    "axes[1,0].set_xticks(range(1, 13))\n",
    "\n",
    "# Correlation between rainfall and water level\n",
    "axes[1,1].scatter(historical_data['rainfall'], historical_data['water_level'], \n",
    "                  alpha=0.5, color='purple')\n",
    "axes[1,1].set_title('Rainfall vs Water Level Correlation', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Rainfall (mm)')\n",
    "axes[1,1].set_ylabel('Water Level (m)')\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = historical_data['rainfall'].corr(historical_data['water_level'])\n",
    "axes[1,1].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "               transform=axes[1,1].transAxes, fontsize=12, \n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"📊 Rainfall-Water Level Correlation: {correlation:.3f}\")\n",
    "print(f\"🌧️ Average Daily Rainfall: {historical_data['rainfall'].mean():.2f} mm\")\n",
    "print(f\"🌊 Average Water Level: {historical_data['water_level'].mean():.2f} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a1b2d1",
   "metadata": {},
   "source": [
    "## 🔧 Data Preprocessing and Feature Engineering\n",
    "\n",
    "This section creates the features needed for flood prediction:\n",
    "1. **Flood Labels**: Binary classification (flood/no flood) based on water level threshold\n",
    "2. **Lag Features**: Rainfall and water level from previous N days\n",
    "3. **Rolling Statistics**: Moving averages and cumulative rainfall\n",
    "4. **Seasonal Features**: Month, season indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Function\n",
    "def create_flood_features(df, flood_threshold=5.5, lag_days=7):\n",
    "    \"\"\"\n",
    "    Create features for flood prediction model\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with 'date', 'rainfall', 'water_level' columns\n",
    "    - flood_threshold: Water level threshold for flood classification (meters)\n",
    "    - lag_days: Number of previous days to include as features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # 1. Create flood label (binary target)\n",
    "    df_features['flood'] = (df_features['water_level'] > flood_threshold).astype(int)\n",
    "    \n",
    "    # 2. Create lag features for rainfall\n",
    "    for i in range(1, lag_days + 1):\n",
    "        df_features[f'rainfall_lag{i}'] = df_features['rainfall'].shift(i)\n",
    "    \n",
    "    # 3. Create lag features for water level\n",
    "    for i in range(1, lag_days + 1):\n",
    "        df_features[f'water_level_lag{i}'] = df_features['water_level'].shift(i)\n",
    "    \n",
    "    # 4. Rolling statistics\n",
    "    df_features['rainfall_3day_avg'] = df_features['rainfall'].rolling(window=3).mean()\n",
    "    df_features['rainfall_7day_avg'] = df_features['rainfall'].rolling(window=7).mean()\n",
    "    df_features['rainfall_3day_sum'] = df_features['rainfall'].rolling(window=3).sum()\n",
    "    df_features['rainfall_7day_sum'] = df_features['rainfall'].rolling(window=7).sum()\n",
    "    \n",
    "    # 5. Water level statistics\n",
    "    df_features['water_level_3day_avg'] = df_features['water_level'].rolling(window=3).mean()\n",
    "    df_features['water_level_7day_max'] = df_features['water_level'].rolling(window=7).max()\n",
    "    df_features['water_level_trend'] = df_features['water_level'] - df_features['water_level_lag1']\n",
    "    \n",
    "    # 6. Seasonal features\n",
    "    df_features['is_monsoon'] = df_features['season'].apply(lambda x: 1 if x == 'monsoon' else 0)\n",
    "    df_features['month_sin'] = np.sin(2 * np.pi * df_features['month'] / 12)\n",
    "    df_features['month_cos'] = np.cos(2 * np.pi * df_features['month'] / 12)\n",
    "    \n",
    "    # 7. Interaction features\n",
    "    df_features['rain_water_interaction'] = df_features['rainfall'] * df_features['water_level_lag1']\n",
    "    \n",
    "    # Drop rows with NaN values (due to lag features)\n",
    "    df_features = df_features.dropna().reset_index(drop=True)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"🔧 Creating features for flood prediction...\")\n",
    "flood_threshold = 5.5  # meters\n",
    "lag_days = 7\n",
    "\n",
    "flood_data = create_flood_features(historical_data, flood_threshold, lag_days)\n",
    "\n",
    "print(f\"📊 Features created! Dataset shape: {flood_data.shape}\")\n",
    "print(f\"🎯 Flood events: {flood_data['flood'].sum()} out of {len(flood_data)} days ({flood_data['flood'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Display feature columns\n",
    "feature_columns = [col for col in flood_data.columns if col not in ['date', 'rainfall', 'water_level', 'month', 'year', 'season']]\n",
    "print(f\"\\n📋 Features created ({len(feature_columns)}):\")\n",
    "for i, col in enumerate(feature_columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# Show sample of engineered features\n",
    "print(\"\\n📋 Sample of engineered features:\")\n",
    "print(flood_data[['date', 'rainfall', 'water_level', 'flood', 'rainfall_lag1', 'rainfall_3day_sum', 'water_level_trend']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e375c3",
   "metadata": {},
   "source": [
    "## 📊 Train-Test Split\n",
    "\n",
    "For time series data, we use chronological split to avoid data leakage:\n",
    "- **Training set**: First 80% of the data (chronologically)\n",
    "- **Test set**: Last 20% of the data\n",
    "- **Features**: All lag and engineered features\n",
    "- **Target**: Binary flood classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df8f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "# Select feature columns (exclude date and original columns)\n",
    "feature_cols = [col for col in flood_data.columns \n",
    "                if col not in ['date', 'rainfall', 'water_level', 'month', 'year', 'season', 'flood']]\n",
    "\n",
    "X = flood_data[feature_cols]\n",
    "y = flood_data['flood']\n",
    "\n",
    "print(f\"🎯 Features selected: {len(feature_cols)}\")\n",
    "print(f\"📊 Feature matrix shape: {X.shape}\")\n",
    "print(f\"🎯 Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Time series split (chronological)\n",
    "split_index = int(len(flood_data) * 0.8)\n",
    "\n",
    "X_train = X.iloc[:split_index]\n",
    "X_test = X.iloc[split_index:]\n",
    "y_train = y.iloc[:split_index]\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "# Get corresponding dates for evaluation\n",
    "train_dates = flood_data['date'].iloc[:split_index]\n",
    "test_dates = flood_data['date'].iloc[split_index:]\n",
    "\n",
    "print(f\"\\n📈 Training set:\")\n",
    "print(f\"   • Size: {len(X_train)} samples\")\n",
    "print(f\"   • Date range: {train_dates.min()} to {train_dates.max()}\")\n",
    "print(f\"   • Flood events: {y_train.sum()} ({y_train.mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🧪 Test set:\")\n",
    "print(f\"   • Size: {len(X_test)} samples\")\n",
    "print(f\"   • Date range: {test_dates.min()} to {test_dates.max()}\")\n",
    "print(f\"   • Flood events: {y_test.sum()} ({y_test.mean()*100:.1f}%)\")\n",
    "\n",
    "# Feature scaling (for LSTM later)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n✅ Data split and scaling completed!\")\n",
    "print(f\"📋 Feature columns: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff51586a",
   "metadata": {},
   "source": [
    "## 🚀 Model Training: XGBoost\n",
    "\n",
    "XGBoost is excellent for tabular data and provides:\n",
    "- High accuracy on structured data\n",
    "- Feature importance rankings\n",
    "- Fast training and prediction\n",
    "- Handles missing values well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost Model\n",
    "print(\"🚀 Training XGBoost model...\")\n",
    "\n",
    "# Configure XGBoost with optimal parameters\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"✅ XGBoost training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb)\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb)\n",
    "auc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "\n",
    "print(f\"\\n📊 XGBoost Performance Metrics:\")\n",
    "print(f\"   • Accuracy:  {accuracy_xgb:.4f}\")\n",
    "print(f\"   • Precision: {precision_xgb:.4f}\")\n",
    "print(f\"   • Recall:    {recall_xgb:.4f}\")\n",
    "print(f\"   • F1-Score:  {f1_xgb:.4f}\")\n",
    "print(f\"   • AUC-ROC:   {auc_xgb:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "print(f\"\\n🔍 Confusion Matrix:\")\n",
    "print(f\"   True Negatives:  {cm_xgb[0,0]}\")\n",
    "print(f\"   False Positives: {cm_xgb[0,1]}\")\n",
    "print(f\"   False Negatives: {cm_xgb[1,0]}\")\n",
    "print(f\"   True Positives:  {cm_xgb[1,1]}\")\n",
    "\n",
    "# Save the model\n",
    "import joblib\n",
    "joblib.dump(xgb_model, 'models/xgboost_flood_model.pkl')\n",
    "joblib.dump(scaler, 'models/feature_scaler.pkl')\n",
    "print(\"\\n💾 XGBoost model saved to 'models/xgboost_flood_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac845018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"🎯 Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='skyblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('XGBoost Feature Importance (Top 15)', fontsize=16, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(top_features['importance']):\n",
    "    plt.text(v + 0.001, i, f'{v:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('models/xgboost_feature_importance.csv', index=False)\n",
    "print(\"💾 Feature importance saved to 'models/xgboost_feature_importance.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb407ec4",
   "metadata": {},
   "source": [
    "## 🧠 Model Training: LSTM\n",
    "\n",
    "LSTM (Long Short-Term Memory) networks are designed for sequence data and can:\n",
    "- Capture temporal dependencies\n",
    "- Learn complex patterns over time\n",
    "- Handle variable-length sequences\n",
    "- Remember important information across time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245059df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM (sequence format)\n",
    "def create_sequences(X, y, window_size=7):\n",
    "    \"\"\"Convert data to sequences for LSTM\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(window_size, len(X)):\n",
    "        X_seq.append(X[i-window_size:i])\n",
    "        y_seq.append(y[i])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Create sequences\n",
    "window_size = 7  # 7-day sequences\n",
    "print(f\"🔄 Creating sequences with window size: {window_size}\")\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train.values, window_size)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test.values, window_size)\n",
    "\n",
    "print(f\"📊 LSTM Training sequences shape: {X_train_seq.shape}\")\n",
    "print(f\"📊 LSTM Test sequences shape: {X_test_seq.shape}\")\n",
    "\n",
    "# Build LSTM Model\n",
    "print(\"🧠 Building LSTM model...\")\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(window_size, len(feature_cols))),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "lstm_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "print(\"📋 LSTM Model Architecture:\")\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train LSTM\n",
    "print(\"🚀 Training LSTM model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    shuffle=False  # Important for time series\n",
    ")\n",
    "\n",
    "training_time_lstm = time.time() - start_time\n",
    "print(f\"✅ LSTM training completed in {training_time_lstm:.2f} seconds\")\n",
    "\n",
    "# Save LSTM model\n",
    "lstm_model.save('models/lstm_flood_model.h5')\n",
    "print(\"💾 LSTM model saved to 'models/lstm_flood_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Predictions and Evaluation\n",
    "y_pred_lstm_proba = lstm_model.predict(X_test_seq)\n",
    "y_pred_lstm = (y_pred_lstm_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate LSTM metrics\n",
    "accuracy_lstm = accuracy_score(y_test_seq, y_pred_lstm)\n",
    "precision_lstm = precision_score(y_test_seq, y_pred_lstm)\n",
    "recall_lstm = recall_score(y_test_seq, y_pred_lstm)\n",
    "f1_lstm = f1_score(y_test_seq, y_pred_lstm)\n",
    "auc_lstm = roc_auc_score(y_test_seq, y_pred_lstm_proba)\n",
    "\n",
    "print(f\"📊 LSTM Performance Metrics:\")\n",
    "print(f\"   • Accuracy:  {accuracy_lstm:.4f}\")\n",
    "print(f\"   • Precision: {precision_lstm:.4f}\")\n",
    "print(f\"   • Recall:    {recall_lstm:.4f}\")\n",
    "print(f\"   • F1-Score:  {f1_lstm:.4f}\")\n",
    "print(f\"   • AUC-ROC:   {auc_lstm:.4f}\")\n",
    "\n",
    "# Model Comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'LSTM'],\n",
    "    'Accuracy': [accuracy_xgb, accuracy_lstm],\n",
    "    'Precision': [precision_xgb, precision_lstm],\n",
    "    'Recall': [recall_xgb, recall_lstm],\n",
    "    'F1-Score': [f1_xgb, f1_lstm],\n",
    "    'AUC-ROC': [auc_xgb, auc_lstm],\n",
    "    'Training Time (s)': [training_time, training_time_lstm]\n",
    "})\n",
    "\n",
    "print(f\"\\n🏆 Model Comparison:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Plot training history for LSTM\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Accuracy\n",
    "axes[0,0].plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "axes[0,0].plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "axes[0,0].set_title('Model Accuracy', fontweight='bold')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[0,1].plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "axes[0,1].plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
    "axes[0,1].set_title('Model Loss', fontweight='bold')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('Loss')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "axes[1,0].plot(history.history['precision'], label='Training Precision', color='blue')\n",
    "axes[1,0].plot(history.history['val_precision'], label='Validation Precision', color='red')\n",
    "axes[1,0].set_title('Model Precision', fontweight='bold')\n",
    "axes[1,0].set_xlabel('Epoch')\n",
    "axes[1,0].set_ylabel('Precision')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1,1].plot(history.history['recall'], label='Training Recall', color='blue')\n",
    "axes[1,1].plot(history.history['val_recall'], label='Validation Recall', color='red')\n",
    "axes[1,1].set_title('Model Recall', fontweight='bold')\n",
    "axes[1,1].set_xlabel('Epoch')\n",
    "axes[1,1].set_ylabel('Recall')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df.to_csv('models/model_comparison.csv', index=False)\n",
    "print(\"💾 Model comparison saved to 'models/model_comparison.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bf4228",
   "metadata": {},
   "source": [
    "## 📈 Model Evaluation and Interpretation\n",
    "\n",
    "Detailed analysis of model performance including:\n",
    "- ROC curves and AUC scores\n",
    "- Confusion matrices\n",
    "- Precision-Recall curves\n",
    "- Feature importance (XGBoost)\n",
    "- Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37531fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Model Evaluation\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# ROC Curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# ROC Curve - XGBoost\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
    "axes[0,0].plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {auc_xgb:.3f})', color='blue', linewidth=2)\n",
    "axes[0,0].plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
    "axes[0,0].set_xlabel('False Positive Rate')\n",
    "axes[0,0].set_ylabel('True Positive Rate')\n",
    "axes[0,0].set_title('ROC Curve - XGBoost', fontweight='bold')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# ROC Curve - LSTM\n",
    "fpr_lstm, tpr_lstm, _ = roc_curve(y_test_seq, y_pred_lstm_proba)\n",
    "axes[0,1].plot(fpr_lstm, tpr_lstm, label=f'LSTM (AUC = {auc_lstm:.3f})', color='red', linewidth=2)\n",
    "axes[0,1].plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
    "axes[0,1].set_xlabel('False Positive Rate')\n",
    "axes[0,1].set_ylabel('True Positive Rate')\n",
    "axes[0,1].set_title('ROC Curve - LSTM', fontweight='bold')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve - XGBoost\n",
    "precision_xgb_curve, recall_xgb_curve, _ = precision_recall_curve(y_test, y_pred_proba_xgb)\n",
    "axes[1,0].plot(recall_xgb_curve, precision_xgb_curve, color='blue', linewidth=2)\n",
    "axes[1,0].set_xlabel('Recall')\n",
    "axes[1,0].set_ylabel('Precision')\n",
    "axes[1,0].set_title('Precision-Recall Curve - XGBoost', fontweight='bold')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve - LSTM\n",
    "precision_lstm_curve, recall_lstm_curve, _ = precision_recall_curve(y_test_seq, y_pred_lstm_proba)\n",
    "axes[1,1].plot(recall_lstm_curve, precision_lstm_curve, color='red', linewidth=2)\n",
    "axes[1,1].set_xlabel('Recall')\n",
    "axes[1,1].set_ylabel('Precision')\n",
    "axes[1,1].set_title('Precision-Recall Curve - LSTM', fontweight='bold')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrices Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# XGBoost Confusion Matrix\n",
    "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('XGBoost Confusion Matrix', fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# LSTM Confusion Matrix\n",
    "cm_lstm = confusion_matrix(y_test_seq, y_pred_lstm)\n",
    "sns.heatmap(cm_lstm, annot=True, fmt='d', cmap='Reds', ax=axes[1])\n",
    "axes[1].set_title('LSTM Confusion Matrix', fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification Reports\n",
    "print(\"📊 XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "print(\"\\n📊 LSTM Classification Report:\")\n",
    "print(classification_report(y_test_seq, y_pred_lstm))\n",
    "\n",
    "# Error Analysis\n",
    "print(\"\\n🔍 Error Analysis:\")\n",
    "print(f\"XGBoost False Positives: {cm_xgb[0,1]} (predicted flood when no flood)\")\n",
    "print(f\"XGBoost False Negatives: {cm_xgb[1,0]} (missed actual floods)\")\n",
    "print(f\"LSTM False Positives: {cm_lstm[0,1]}\")\n",
    "print(f\"LSTM False Negatives: {cm_lstm[1,0]}\")\n",
    "\n",
    "# Best model selection\n",
    "if f1_xgb > f1_lstm:\n",
    "    best_model = \"XGBoost\"\n",
    "    best_f1 = f1_xgb\n",
    "else:\n",
    "    best_model = \"LSTM\"\n",
    "    best_f1 = f1_lstm\n",
    "\n",
    "print(f\"\\n🏆 Best performing model: {best_model} (F1-Score: {best_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7587879",
   "metadata": {},
   "source": [
    "## 🌦️ Live Rainfall Data Fetching via API\n",
    "\n",
    "Integration with weather APIs to get real-time rainfall data:\n",
    "- **OpenWeatherMap**: Historical and current weather data\n",
    "- **MeteoSource**: High-resolution weather data for Bangladesh\n",
    "- **Automatic data processing** for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647268be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live Rainfall Data Fetching Functions\n",
    "\n",
    "def fetch_openweather_historical(lat, lon, days=7, api_key=OPENWEATHER_API_KEY):\n",
    "    \"\"\"\n",
    "    Fetch historical rainfall data from OpenWeatherMap API\n",
    "    \"\"\"\n",
    "    if api_key == \"YOUR_OPENWEATHER_API_KEY\":\n",
    "        print(\"⚠️ Please set your OpenWeatherMap API key!\")\n",
    "        return None\n",
    "    \n",
    "    rainfall_data = []\n",
    "    \n",
    "    for i in range(days):\n",
    "        # Get timestamp for each day\n",
    "        target_date = datetime.now() - timedelta(days=i)\n",
    "        timestamp = int(target_date.timestamp())\n",
    "        \n",
    "        url = f\"{OPENWEATHER_BASE_URL}?lat={lat}&lon={lon}&dt={timestamp}&appid={api_key}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Extract rainfall data\n",
    "                hourly_data = data.get('hourly', [])\n",
    "                daily_rain = 0\n",
    "                \n",
    "                for hour in hourly_data:\n",
    "                    rain_data = hour.get('rain', {})\n",
    "                    daily_rain += rain_data.get('1h', 0)  # 1-hour rainfall in mm\n",
    "                \n",
    "                rainfall_data.append({\n",
    "                    'date': target_date.strftime('%Y-%m-%d'),\n",
    "                    'rainfall': daily_rain\n",
    "                })\n",
    "            else:\n",
    "                print(f\"❌ API Error for {target_date.strftime('%Y-%m-%d')}: {data.get('message', 'Unknown error')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Network error: {str(e)}\")\n",
    "    \n",
    "    return pd.DataFrame(rainfall_data).sort_values('date')\n",
    "\n",
    "def fetch_meteosource_data(lat, lon, api_key=METEOSOURCE_API_KEY):\n",
    "    \"\"\"\n",
    "    Fetch current and forecast data from MeteoSource API\n",
    "    \"\"\"\n",
    "    if api_key == \"YOUR_METEOSOURCE_API_KEY\":\n",
    "        print(\"⚠️ Please set your MeteoSource API key!\")\n",
    "        return None\n",
    "    \n",
    "    url = f\"{METEOSOURCE_BASE_URL}?lat={lat}&lon={lon}&sections=current,daily&timezone=UTC&language=en&units=metric&key={api_key}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        data = response.json()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            daily_data = data.get('daily', {}).get('data', [])\n",
    "            \n",
    "            rainfall_data = []\n",
    "            for day in daily_data[:7]:  # Last 7 days\n",
    "                rainfall_data.append({\n",
    "                    'date': day['day'],\n",
    "                    'rainfall': day.get('precipitation', {}).get('total', 0)\n",
    "                })\n",
    "            \n",
    "            return pd.DataFrame(rainfall_data)\n",
    "        else:\n",
    "            print(f\"❌ MeteoSource API Error: {data.get('message', 'Unknown error')}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Network error: {str(e)}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_simulated_live_data(location='Dhaka', days=7):\n",
    "    \"\"\"\n",
    "    Simulate live rainfall data for demonstration\n",
    "    (Use when API keys are not available)\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Simulating live rainfall data for {location}...\")\n",
    "    \n",
    "    dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days)]\n",
    "    dates.reverse()\n",
    "    \n",
    "    # Simulate realistic rainfall patterns\n",
    "    np.random.seed(int(datetime.now().timestamp()) % 1000)\n",
    "    rainfall = np.random.gamma(2, 3, days)  # Gamma distribution\n",
    "    rainfall = np.maximum(rainfall, 0)  # No negative values\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'rainfall': rainfall\n",
    "    })\n",
    "\n",
    "# Fetch Live Rainfall Data\n",
    "location = 'Dhaka'\n",
    "lat, lon = LOCATIONS[location]\n",
    "\n",
    "print(f\"🌦️ Fetching live rainfall data for {location} ({lat}, {lon})\")\n",
    "\n",
    "# Try OpenWeatherMap first, then MeteoSource, then simulation\n",
    "live_rainfall = fetch_openweather_historical(lat, lon, days=7)\n",
    "\n",
    "if live_rainfall is None:\n",
    "    live_rainfall = fetch_meteosource_data(lat, lon)\n",
    "\n",
    "if live_rainfall is None:\n",
    "    live_rainfall = get_simulated_live_data(location, days=7)\n",
    "\n",
    "print(f\"✅ Retrieved {len(live_rainfall)} days of rainfall data\")\n",
    "print(\"\\n📊 Live Rainfall Data:\")\n",
    "print(live_rainfall)\n",
    "\n",
    "# Add water level estimates (simplified simulation)\n",
    "# In production, this would come from river monitoring stations\n",
    "base_level = 4.2\n",
    "live_rainfall['estimated_water_level'] = base_level + (live_rainfall['rainfall'] * 0.1) + np.random.normal(0, 0.2, len(live_rainfall))\n",
    "live_rainfall['estimated_water_level'] = np.maximum(live_rainfall['estimated_water_level'], 2.0)\n",
    "\n",
    "print(\"\\n🌊 With estimated water levels:\")\n",
    "print(live_rainfall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9153bd",
   "metadata": {},
   "source": [
    "## 🎯 Predicting Floods with Latest Data\n",
    "\n",
    "Using the trained models to predict flood risk based on the latest rainfall data:\n",
    "1. **Feature Engineering**: Create lag features from live data\n",
    "2. **Model Prediction**: Use both XGBoost and LSTM models\n",
    "3. **Risk Assessment**: Combine predictions for final assessment\n",
    "4. **Confidence Scoring**: Provide confidence levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e65d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live Flood Prediction Function\n",
    "def predict_flood_risk(rainfall_data, models_dict, scaler, feature_cols, flood_threshold=5.5):\n",
    "    \"\"\"\n",
    "    Predict flood risk using live rainfall data\n",
    "    \n",
    "    Parameters:\n",
    "    - rainfall_data: DataFrame with 'date', 'rainfall', 'estimated_water_level'\n",
    "    - models_dict: Dictionary containing trained models\n",
    "    - scaler: Fitted StandardScaler\n",
    "    - feature_cols: List of feature column names\n",
    "    - flood_threshold: Water level threshold for flood\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare data similar to training\n",
    "    df = rainfall_data.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Add temporal features\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['season'] = df['month'].apply(lambda x: 'monsoon' if 6 <= x <= 9 else 'dry')\n",
    "    \n",
    "    # Create same features as training\n",
    "    try:\n",
    "        df_features = create_flood_features(df.rename(columns={'estimated_water_level': 'water_level'}), \n",
    "                                          flood_threshold, lag_days=7)\n",
    "        \n",
    "        if len(df_features) == 0:\n",
    "            return None, \"Not enough data for prediction\"\n",
    "        \n",
    "        # Get the latest row for prediction\n",
    "        latest_features = df_features[feature_cols].iloc[-1:].values\n",
    "        latest_features_scaled = scaler.transform(latest_features)\n",
    "        \n",
    "        predictions = {}\n",
    "        \n",
    "        # XGBoost prediction\n",
    "        if 'xgboost' in models_dict:\n",
    "            xgb_prob = models_dict['xgboost'].predict_proba(latest_features)[0, 1]\n",
    "            xgb_pred = models_dict['xgboost'].predict(latest_features)[0]\n",
    "            predictions['xgboost'] = {\n",
    "                'probability': xgb_prob,\n",
    "                'prediction': xgb_pred,\n",
    "                'confidence': max(xgb_prob, 1-xgb_prob)  # Distance from 0.5\n",
    "            }\n",
    "        \n",
    "        # LSTM prediction (if we have enough sequence data)\n",
    "        if 'lstm' in models_dict and len(df_features) >= 7:\n",
    "            # Create sequence for LSTM\n",
    "            sequence_data = df_features[feature_cols].iloc[-7:].values\n",
    "            sequence_scaled = scaler.transform(sequence_data)\n",
    "            sequence_input = sequence_scaled.reshape(1, 7, len(feature_cols))\n",
    "            \n",
    "            lstm_prob = models_dict['lstm'].predict(sequence_input)[0, 0]\n",
    "            lstm_pred = int(lstm_prob > 0.5)\n",
    "            predictions['lstm'] = {\n",
    "                'probability': lstm_prob,\n",
    "                'prediction': lstm_pred,\n",
    "                'confidence': max(lstm_prob, 1-lstm_prob)\n",
    "            }\n",
    "        \n",
    "        # Ensemble prediction (average of available models)\n",
    "        if predictions:\n",
    "            avg_prob = np.mean([p['probability'] for p in predictions.values()])\n",
    "            ensemble_pred = int(avg_prob > 0.5)\n",
    "            ensemble_confidence = max(avg_prob, 1-avg_prob)\n",
    "            \n",
    "            predictions['ensemble'] = {\n",
    "                'probability': avg_prob,\n",
    "                'prediction': ensemble_pred,\n",
    "                'confidence': ensemble_confidence\n",
    "            }\n",
    "        \n",
    "        return predictions, df_features.iloc[-1]\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, f\"Error in prediction: {str(e)}\"\n",
    "\n",
    "# Load trained models\n",
    "try:\n",
    "    models = {\n",
    "        'xgboost': joblib.load('models/xgboost_flood_model.pkl'),\n",
    "        'lstm': tf.keras.models.load_model('models/lstm_flood_model.h5')\n",
    "    }\n",
    "    loaded_scaler = joblib.load('models/feature_scaler.pkl')\n",
    "    print(\"✅ Models loaded successfully!\")\n",
    "except:\n",
    "    # Use the models we just trained\n",
    "    models = {\n",
    "        'xgboost': xgb_model,\n",
    "        'lstm': lstm_model\n",
    "    }\n",
    "    loaded_scaler = scaler\n",
    "    print(\"✅ Using freshly trained models!\")\n",
    "\n",
    "# Make Prediction\n",
    "print(f\"\\n🔮 Making flood prediction for {location}...\")\n",
    "predictions, latest_data = predict_flood_risk(\n",
    "    live_rainfall, models, loaded_scaler, feature_cols, \n",
    "    flood_threshold=FLOOD_THRESHOLDS.get(location, 5.5)\n",
    ")\n",
    "\n",
    "if predictions:\n",
    "    print(f\"\\n🎯 Flood Prediction Results for {location}:\")\n",
    "    print(f\"📅 Latest data date: {live_rainfall['date'].iloc[-1]}\")\n",
    "    print(f\"🌧️ Recent rainfall: {live_rainfall['rainfall'].iloc[-1]:.1f} mm\")\n",
    "    print(f\"🌊 Estimated water level: {live_rainfall['estimated_water_level'].iloc[-1]:.2f} m\")\n",
    "    print(f\"🚨 Flood threshold: {FLOOD_THRESHOLDS.get(location, 5.5)} m\")\n",
    "    \n",
    "    print(f\"\\n📊 Model Predictions:\")\n",
    "    for model_name, pred in predictions.items():\n",
    "        risk_level = \"🔴 HIGH RISK\" if pred['prediction'] == 1 else \"🟢 LOW RISK\"\n",
    "        print(f\"   • {model_name.upper()}: {risk_level}\")\n",
    "        print(f\"     - Probability: {pred['probability']:.3f}\")\n",
    "        print(f\"     - Confidence: {pred['confidence']:.3f}\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    ensemble_pred = predictions.get('ensemble', predictions[list(predictions.keys())[0]])\n",
    "    \n",
    "    if ensemble_pred['prediction'] == 1:\n",
    "        print(f\"\\n🚨 FLOOD WARNING!\")\n",
    "        print(f\"   Risk Level: HIGH ({ensemble_pred['probability']:.1%})\")\n",
    "        print(f\"   Confidence: {ensemble_pred['confidence']:.1%}\")\n",
    "        alert_needed = True\n",
    "    else:\n",
    "        print(f\"\\n✅ No immediate flood risk\")\n",
    "        print(f\"   Risk Level: LOW ({ensemble_pred['probability']:.1%})\")\n",
    "        print(f\"   Confidence: {ensemble_pred['confidence']:.1%}\")\n",
    "        alert_needed = False\n",
    "        \n",
    "else:\n",
    "    print(f\"❌ Could not make prediction: {latest_data}\")\n",
    "    alert_needed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de655fcd",
   "metadata": {},
   "source": [
    "## 📝 CSV Logger for Daily Predictions\n",
    "\n",
    "Automatic logging system to track predictions over time:\n",
    "- **Prediction History**: Store all predictions with timestamps\n",
    "- **Performance Tracking**: Monitor accuracy over time\n",
    "- **Data Backup**: Keep records for analysis and model improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b1168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Logger for Predictions\n",
    "def log_prediction(location, rainfall_data, predictions, log_file='logs/flood_predictions.csv'):\n",
    "    \"\"\"\n",
    "    Log prediction results to CSV file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create log entry\n",
    "    log_entry = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'location': location,\n",
    "        'date': rainfall_data['date'].iloc[-1],\n",
    "        'recent_rainfall': rainfall_data['rainfall'].iloc[-1],\n",
    "        'estimated_water_level': rainfall_data['estimated_water_level'].iloc[-1],\n",
    "        'flood_threshold': FLOOD_THRESHOLDS.get(location, 5.5)\n",
    "    }\n",
    "    \n",
    "    # Add model predictions\n",
    "    if predictions:\n",
    "        for model_name, pred in predictions.items():\n",
    "            log_entry[f'{model_name}_probability'] = pred['probability']\n",
    "            log_entry[f'{model_name}_prediction'] = pred['prediction']\n",
    "            log_entry[f'{model_name}_confidence'] = pred['confidence']\n",
    "    \n",
    "    # Create DataFrame\n",
    "    log_df = pd.DataFrame([log_entry])\n",
    "    \n",
    "    # Check if log file exists\n",
    "    if os.path.exists(log_file):\n",
    "        # Append to existing file\n",
    "        log_df.to_csv(log_file, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Create new file with headers\n",
    "        log_df.to_csv(log_file, mode='w', header=True, index=False)\n",
    "    \n",
    "    print(f\"📝 Prediction logged to {log_file}\")\n",
    "\n",
    "def view_prediction_history(log_file='logs/flood_predictions.csv', days=30):\n",
    "    \"\"\"\n",
    "    View recent prediction history\n",
    "    \"\"\"\n",
    "    if not os.path.exists(log_file):\n",
    "        print(\"📭 No prediction history found\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(log_file)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Filter recent predictions\n",
    "    cutoff_date = datetime.now() - timedelta(days=days)\n",
    "    recent_df = df[df['timestamp'] >= cutoff_date].copy()\n",
    "    \n",
    "    if len(recent_df) == 0:\n",
    "        print(f\"📭 No predictions found in the last {days} days\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"📊 Prediction History (Last {days} days):\")\n",
    "    print(f\"   • Total predictions: {len(recent_df)}\")\n",
    "    \n",
    "    if 'ensemble_prediction' in recent_df.columns:\n",
    "        flood_predictions = recent_df['ensemble_prediction'].sum()\n",
    "        print(f\"   • Flood warnings: {flood_predictions}\")\n",
    "        print(f\"   • Warning rate: {flood_predictions/len(recent_df)*100:.1f}%\")\n",
    "    \n",
    "    return recent_df\n",
    "\n",
    "# Log Current Prediction\n",
    "if predictions:\n",
    "    log_prediction(location, live_rainfall, predictions)\n",
    "    \n",
    "    # View recent history\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    history = view_prediction_history(days=30)\n",
    "    \n",
    "    if history is not None and len(history) > 1:\n",
    "        print(\"\\n📈 Recent Prediction Trends:\")\n",
    "        \n",
    "        # Plot prediction history\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Rainfall and water level trends\n",
    "        axes[0].plot(pd.to_datetime(history['date']), history['recent_rainfall'], \n",
    "                    marker='o', label='Rainfall (mm)', color='blue', alpha=0.7)\n",
    "        ax0_twin = axes[0].twinx()\n",
    "        ax0_twin.plot(pd.to_datetime(history['date']), history['estimated_water_level'], \n",
    "                     marker='s', label='Water Level (m)', color='green', alpha=0.7)\n",
    "        ax0_twin.axhline(y=history['flood_threshold'].iloc[0], color='red', \n",
    "                        linestyle='--', label='Flood Threshold')\n",
    "        \n",
    "        axes[0].set_xlabel('Date')\n",
    "        axes[0].set_ylabel('Rainfall (mm)', color='blue')\n",
    "        ax0_twin.set_ylabel('Water Level (m)', color='green')\n",
    "        axes[0].set_title('Rainfall and Water Level Trends', fontweight='bold')\n",
    "        axes[0].legend(loc='upper left')\n",
    "        ax0_twin.legend(loc='upper right')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Prediction probabilities\n",
    "        if 'ensemble_probability' in history.columns:\n",
    "            axes[1].plot(pd.to_datetime(history['date']), history['ensemble_probability'], \n",
    "                        marker='o', color='red', linewidth=2, label='Flood Probability')\n",
    "            axes[1].axhline(y=0.5, color='black', linestyle='--', alpha=0.5, label='Decision Threshold')\n",
    "            axes[1].fill_between(pd.to_datetime(history['date']), history['ensemble_probability'], \n",
    "                               0.5, where=(history['ensemble_probability'] > 0.5), \n",
    "                               color='red', alpha=0.3, label='High Risk')\n",
    "        \n",
    "        axes[1].set_xlabel('Date')\n",
    "        axes[1].set_ylabel('Flood Probability')\n",
    "        axes[1].set_title('Flood Risk Predictions Over Time', fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].set_ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Set up automated logging (for production)\n",
    "def setup_automated_logging():\n",
    "    \"\"\"\n",
    "    Set up automated logging that runs daily\n",
    "    This would typically be called by a cron job or scheduler\n",
    "    \"\"\"\n",
    "    \n",
    "    script_content = f'''#!/usr/bin/env python3\n",
    "import sys\n",
    "sys.path.append('{os.getcwd()}')\n",
    "\n",
    "# Import required modules and run prediction\n",
    "from flood_prediction_system import *\n",
    "\n",
    "# Run daily prediction and logging\n",
    "location = \"Dhaka\"\n",
    "lat, lon = LOCATIONS[location]\n",
    "live_data = get_simulated_live_data(location, days=7)\n",
    "predictions, _ = predict_flood_risk(live_data, models, scaler, feature_cols)\n",
    "\n",
    "if predictions:\n",
    "    log_prediction(location, live_data, predictions)\n",
    "    print(f\"Daily prediction logged for {{location}}\")\n",
    "'''\n",
    "    \n",
    "    with open('logs/daily_prediction.py', 'w') as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    print(\"📅 Automated logging script created: 'logs/daily_prediction.py'\")\n",
    "    print(\"💡 To run daily, add to crontab: 0 6 * * * python3 /path/to/logs/daily_prediction.py\")\n",
    "\n",
    "setup_automated_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3a092",
   "metadata": {},
   "source": [
    "## 🚨 Telegram/Email Alert Integration\n",
    "\n",
    "Automated alert system that sends notifications when flood risk is detected:\n",
    "- **Telegram Bot**: Instant messaging alerts\n",
    "- **Email Notifications**: Detailed reports via email\n",
    "- **Smart Alerting**: Prevents spam with cooldown periods\n",
    "- **Rich Content**: Maps, charts, and detailed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc138c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alert System Implementation\n",
    "\n",
    "def send_telegram_alert(message, bot_token=TELEGRAM_BOT_TOKEN, chat_id=TELEGRAM_CHAT_ID):\n",
    "    \"\"\"\n",
    "    Send alert via Telegram Bot\n",
    "    \"\"\"\n",
    "    if bot_token == \"YOUR_TELEGRAM_BOT_TOKEN\" or chat_id == \"YOUR_TELEGRAM_CHAT_ID\":\n",
    "        print(\"⚠️ Telegram credentials not configured. Skipping Telegram alert.\")\n",
    "        return False\n",
    "    \n",
    "    url = f\"https://api.telegram.org/bot{bot_token}/sendMessage\"\n",
    "    \n",
    "    payload = {\n",
    "        'chat_id': chat_id,\n",
    "        'text': message,\n",
    "        'parse_mode': 'Markdown'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(\"✅ Telegram alert sent successfully!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Telegram alert failed: {response.text}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Telegram error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def send_email_alert(subject, body, config=EMAIL_CONFIG):\n",
    "    \"\"\"\n",
    "    Send alert via Email\n",
    "    \"\"\"\n",
    "    if config['sender_email'] == 'YOUR_EMAIL@gmail.com':\n",
    "        print(\"⚠️ Email credentials not configured. Skipping email alert.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Create message\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = config['sender_email']\n",
    "        msg['To'] = config['recipient_email']\n",
    "        msg['Subject'] = subject\n",
    "        \n",
    "        msg.attach(MIMEText(body, 'plain'))\n",
    "        \n",
    "        # Send email\n",
    "        server = smtplib.SMTP(config['smtp_server'], config['port'])\n",
    "        server.starttls()\n",
    "        server.login(config['sender_email'], config['sender_password'])\n",
    "        \n",
    "        text = msg.as_string()\n",
    "        server.sendmail(config['sender_email'], config['recipient_email'], text)\n",
    "        server.quit()\n",
    "        \n",
    "        print(\"✅ Email alert sent successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Email error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def create_flood_alert_message(location, predictions, rainfall_data, threshold):\n",
    "    \"\"\"\n",
    "    Create formatted alert message\n",
    "    \"\"\"\n",
    "    ensemble_pred = predictions.get('ensemble', predictions[list(predictions.keys())[0]])\n",
    "    \n",
    "    # Emoji based on risk level\n",
    "    risk_emoji = \"🚨🔴\" if ensemble_pred['prediction'] == 1 else \"✅🟢\"\n",
    "    \n",
    "    message = f\"\"\"\n",
    "{risk_emoji} **FLOOD ALERT - {location.upper()}** {risk_emoji}\n",
    "\n",
    "📅 **Date**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n",
    "📍 **Location**: {location}\n",
    "🌧️ **Recent Rainfall**: {rainfall_data['rainfall'].iloc[-1]:.1f} mm\n",
    "🌊 **Water Level**: {rainfall_data['estimated_water_level'].iloc[-1]:.2f} m\n",
    "🚨 **Flood Threshold**: {threshold} m\n",
    "\n",
    "📊 **AI Predictions**:\n",
    "\"\"\"\n",
    "    \n",
    "    for model_name, pred in predictions.items():\n",
    "        risk_text = \"HIGH RISK\" if pred['prediction'] == 1 else \"LOW RISK\"\n",
    "        message += f\"• {model_name.upper()}: {risk_text} ({pred['probability']:.1%})\\n\"\n",
    "    \n",
    "    if ensemble_pred['prediction'] == 1:\n",
    "        message += f\"\"\"\n",
    "⚠️ **FLOOD WARNING ISSUED**\n",
    "Risk Level: HIGH ({ensemble_pred['probability']:.1%})\n",
    "Confidence: {ensemble_pred['confidence']:.1%}\n",
    "\n",
    "🏃‍♂️ **Recommended Actions**:\n",
    "• Monitor water levels closely\n",
    "• Prepare evacuation if necessary\n",
    "• Move valuables to higher ground\n",
    "• Stay informed through official channels\n",
    "\"\"\"\n",
    "    else:\n",
    "        message += f\"\"\"\n",
    "✅ **No Immediate Flood Risk**\n",
    "Risk Level: LOW ({ensemble_pred['probability']:.1%})\n",
    "Confidence: {ensemble_pred['confidence']:.1%}\n",
    "\n",
    "📋 **Current Status**: Normal\n",
    "Continue monitoring conditions.\n",
    "\"\"\"\n",
    "    \n",
    "    message += f\"\\\\n🤖 *Generated by AI Flood Prediction System*\"\n",
    "    \n",
    "    return message\n",
    "\n",
    "def check_alert_cooldown(location, cooldown_hours=6):\n",
    "    \"\"\"\n",
    "    Check if enough time has passed since last alert to prevent spam\n",
    "    \"\"\"\n",
    "    cooldown_file = f'alerts/last_alert_{location.lower()}.txt'\n",
    "    \n",
    "    if os.path.exists(cooldown_file):\n",
    "        try:\n",
    "            with open(cooldown_file, 'r') as f:\n",
    "                last_alert_time = datetime.fromisoformat(f.read().strip())\n",
    "            \n",
    "            time_diff = datetime.now() - last_alert_time\n",
    "            if time_diff.total_seconds() < cooldown_hours * 3600:\n",
    "                remaining = cooldown_hours * 3600 - time_diff.total_seconds()\n",
    "                print(f\"⏰ Alert cooldown active. {remaining/3600:.1f} hours remaining.\")\n",
    "                return False\n",
    "        except:\n",
    "            pass  # File corrupted or doesn't exist\n",
    "    \n",
    "    # Update last alert time\n",
    "    with open(cooldown_file, 'w') as f:\n",
    "        f.write(datetime.now().isoformat())\n",
    "    \n",
    "    return True\n",
    "\n",
    "def send_flood_alert(location, predictions, rainfall_data, threshold, force=False):\n",
    "    \"\"\"\n",
    "    Send flood alert via all configured channels\n",
    "    \"\"\"\n",
    "    if not force and not check_alert_cooldown(location, cooldown_hours=6):\n",
    "        return False\n",
    "    \n",
    "    # Create alert message\n",
    "    message = create_flood_alert_message(location, predictions, rainfall_data, threshold)\n",
    "    \n",
    "    print(f\"🚨 Sending flood alert for {location}...\")\n",
    "    \n",
    "    # Send alerts\n",
    "    telegram_sent = send_telegram_alert(message)\n",
    "    \n",
    "    # Email with more detailed info\n",
    "    email_subject = f\"🚨 Flood Alert: {location} - {datetime.now().strftime('%Y-%m-%d')}\"\n",
    "    email_sent = send_email_alert(email_subject, message.replace('*', '').replace('`', ''))\n",
    "    \n",
    "    if telegram_sent or email_sent:\n",
    "        print(\"✅ Alert sent successfully!\")\n",
    "        \n",
    "        # Log alert\n",
    "        alert_log = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'location': location,\n",
    "            'alert_type': 'flood_warning' if predictions.get('ensemble', {}).get('prediction', 0) == 1 else 'status_update',\n",
    "            'telegram_sent': telegram_sent,\n",
    "            'email_sent': email_sent,\n",
    "            'risk_probability': predictions.get('ensemble', {}).get('probability', 0)\n",
    "        }\n",
    "        \n",
    "        alert_df = pd.DataFrame([alert_log])\n",
    "        alert_file = 'alerts/alert_history.csv'\n",
    "        \n",
    "        if os.path.exists(alert_file):\n",
    "            alert_df.to_csv(alert_file, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            alert_df.to_csv(alert_file, mode='w', header=True, index=False)\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ Failed to send alerts\")\n",
    "        return False\n",
    "\n",
    "# Send Alert if High Risk Detected\n",
    "if predictions and alert_needed:\n",
    "    threshold = FLOOD_THRESHOLDS.get(location, 5.5)\n",
    "    alert_success = send_flood_alert(location, predictions, live_rainfall, threshold)\n",
    "    \n",
    "    if alert_success:\n",
    "        print(\"🚨 Emergency alert sent to all configured channels!\")\n",
    "    else:\n",
    "        print(\"⚠️ Alert system encountered issues\")\n",
    "        \n",
    "elif predictions:\n",
    "    print(\"📱 No alert needed - flood risk is low\")\n",
    "    \n",
    "    # Optional: Send daily status update (uncomment to enable)\n",
    "    # send_flood_alert(location, predictions, live_rainfall, threshold, force=True)\n",
    "\n",
    "# View Alert History\n",
    "def view_alert_history(days=30):\n",
    "    \"\"\"View recent alert history\"\"\"\n",
    "    alert_file = 'alerts/alert_history.csv'\n",
    "    \n",
    "    if not os.path.exists(alert_file):\n",
    "        print(\"📭 No alert history found\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_csv(alert_file)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Filter recent alerts\n",
    "    cutoff_date = datetime.now() - timedelta(days=days)\n",
    "    recent_df = df[df['timestamp'] >= cutoff_date]\n",
    "    \n",
    "    if len(recent_df) == 0:\n",
    "        print(f\"📭 No alerts sent in the last {days} days\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📊 Alert History (Last {days} days):\")\n",
    "    print(f\"   • Total alerts: {len(recent_df)}\")\n",
    "    print(f\"   • Flood warnings: {(recent_df['alert_type'] == 'flood_warning').sum()}\")\n",
    "    print(f\"   • Status updates: {(recent_df['alert_type'] == 'status_update').sum()}\")\n",
    "    print(f\"   • Telegram success rate: {recent_df['telegram_sent'].mean()*100:.1f}%\")\n",
    "    print(f\"   • Email success rate: {recent_df['email_sent'].mean()*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\\\n📋 Recent Alerts:\")\n",
    "    for _, row in recent_df.tail(5).iterrows():\n",
    "        alert_type = \"🚨\" if row['alert_type'] == 'flood_warning' else \"📊\"\n",
    "        print(f\"   {alert_type} {row['timestamp'].strftime('%Y-%m-%d %H:%M')} - {row['location']} ({row['risk_probability']:.1%})\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "view_alert_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ef160",
   "metadata": {},
   "source": [
    "## 🎉 System Summary and Next Steps\n",
    "\n",
    "### ✅ What We've Built:\n",
    "\n",
    "1. **🤖 AI Models**: XGBoost and LSTM models for flood prediction\n",
    "2. **📡 Live Data**: API integration for real-time rainfall data\n",
    "3. **🚨 Alert System**: Telegram and email notifications\n",
    "4. **📊 Logging**: CSV tracking of predictions and alerts\n",
    "5. **📈 Monitoring**: Historical analysis and trend visualization\n",
    "\n",
    "### 🚀 Deployment Ready Features:\n",
    "\n",
    "- **Production Models**: Trained and saved models ready for deployment\n",
    "- **API Integration**: OpenWeatherMap and MeteoSource support\n",
    "- **Automated Logging**: Daily prediction tracking\n",
    "- **Smart Alerts**: Cooldown system to prevent spam\n",
    "- **Error Handling**: Robust error handling and fallbacks\n",
    "\n",
    "### 📝 To Get Started:\n",
    "\n",
    "1. **Get API Keys**:\n",
    "   - OpenWeatherMap: https://openweathermap.org/api\n",
    "   - MeteoSource: https://www.meteosource.com/\n",
    "\n",
    "2. **Set up Alerts**:\n",
    "   - Create Telegram Bot: @BotFather on Telegram\n",
    "   - Configure email settings (Gmail App Password recommended)\n",
    "\n",
    "3. **Deploy**:\n",
    "   - Set up cron job for daily predictions\n",
    "   - Monitor logs and adjust thresholds as needed\n",
    "\n",
    "### 🔧 Customization Options:\n",
    "\n",
    "- **Add More Locations**: Update `LOCATIONS` and `FLOOD_THRESHOLDS`\n",
    "- **Adjust Models**: Retrain with local data for better accuracy\n",
    "- **Custom Alerts**: Modify alert messages and delivery methods\n",
    "- **Dashboard**: Create web interface for real-time monitoring\n",
    "\n",
    "---\n",
    "\n",
    "**🌊 Your Bangladesh Flood Prediction System is ready to save lives!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
